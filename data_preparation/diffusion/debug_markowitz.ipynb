{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49e78b13",
   "metadata": {},
   "source": [
    "# Markowitz Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "id": "cde2d656",
   "metadata": {},
   "source": [
    "from global_import import *\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ace88968",
   "metadata": {},
   "source": [
    "# Platform Initialization"
   ]
  },
  {
   "cell_type": "code",
   "id": "dfbfe7e0",
   "metadata": {},
   "source": [
    "project_tourism = dh.get_or_create_project(\"overtourism1\")\n",
    "endpoint_url=\"http://minio:9000\"\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                    endpoint_url=endpoint_url)\n",
    "\n",
    "bucket = s3.Bucket('datalake')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a61cda2d",
   "metadata": {},
   "source": [
    "# Custom Parameters "
   ]
  },
  {
   "cell_type": "code",
   "id": "72b1e963",
   "metadata": {},
   "source": [
    "column_presences_2_hold = [\"AREA_ID\",\"TIME_BLOCK_ID\",col_str_day_od,\"PRESENCES\"]\n",
    "# Compute the total number of presences for each AREA_ID\n",
    "hour_ids = [7,8,10,16,18] \n",
    "hour_id = hour_ids[0]\n",
    "nation = None               \n",
    "# NOTE: in aggregate_presences                                                                        # None means all nations    \n",
    "col_total_presences_oct_no_hour = f\"total_presences_oct_no_hour_{hour_id}\"                          # total presences in October without filtering by hour\n",
    "col_total_presences_tour_no_hour = f\"total_presences_no_hour_{hour_id}\"                             # total presences (touristic months) without filtering by hour\n",
    "# NOTE: To insert inside loop in hours\n",
    "col_total_presences_tour = f\"total_presences_{hour_id}\"                                             # total presences at hour_id\n",
    "# NOTE: When starting Markowitz√π\n",
    "col_tot_diff_oct = f\"total_diff_oct_{hour_id}\"                                                      # total difference October                   \n",
    "col_tot_diff_october_mean_0 = f\"total_diff_october_mean_0_{hour_id}\"                                # total difference October - mean 0   \n",
    "col_tot_diff_october_mean_0_var_1 = f\"total_diff_october_mean_0_var_1_{hour_id}\"                    # total difference October - mean 0 - var 1  (For random matrix standardization)\n",
    "str_column_cov = \"cov\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "21f89d4f",
   "metadata": {},
   "source": [
    "loops = {\n",
    "    \"day\": [\"Monday\",\"Tuesday\"],\n",
    "    \"hour\": [7,8],\n",
    "    \"user\": [\"a\",\"b\"],\n",
    "    \"weekday\": [\"is\", \"is_not\"],\n",
    "    }\n",
    "\n",
    "# Figure out which iterables to use based on the case_pipeline name\n",
    "active_dimensions = [key for key in loops if key in \"day_hour\"]\n",
    "\n",
    "active_dimensions\n",
    "import itertools\n",
    "all_combinations = itertools.product(*[loops[k] for k in active_dimensions])\n",
    "for combination in all_combinations:\n",
    "    print(combination)\n",
    "    kwargs = {}\n",
    "    # Fill kwargs dynamically {}\n",
    "    for dim, value in zip(active_dimensions, combination):\n",
    "        kwargs[dim] = value\n",
    "    print(\"kwargs:\", kwargs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dafde8ad",
   "metadata": {},
   "source": [
    "# Extract days available"
   ]
  },
  {
   "cell_type": "code",
   "id": "796ecc7d",
   "metadata": {},
   "source": [
    "print(\"Extract list of files from bucket...\")\n",
    "list_files_od, list_files_presenze, list_str_dates_yyyymm = extract_filenames_and_date_from_bucket(bucket)\n",
    "date_in_file_2_skip = {'projects/tourism/_origin/vodafone-aixpa/od-mask_202407.parquet':\"2024-08-08\",\n",
    "                       'projects/tourism/_origin/vodafone-aixpa/od-mask_202408.parquet':\"2024-07-23\"}\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "37c246d4",
   "metadata": {},
   "source": [
    "# Presences October"
   ]
  },
  {
   "cell_type": "code",
   "id": "872c9c4e",
   "metadata": {},
   "source": [
    "# NOTE: Extract the null day\n",
    "print(\"Initialize null day OD-presenze...\")\n",
    "df_presenze_null_days = extract_presences_vodafone_from_bucket(s3,list_files_presenze, 2)                                                                                                      # NOTE: download the file from the bucket\n",
    "\n",
    "df_presenze_null_days = add_is_weekday_from_period_presenze_null_days(df_presenze_null_days, period_col=\"PERIOD_ID\", is_weekday_col=\"is_weekday\")\n",
    "\n",
    "print(\"Compute the total number of presences for each AREA_ID...\")\n",
    "col_str_average_presences_null_day = \"avg_presences_october_2024\"\n",
    "df_presenze_null_days = compute_presences_average(\n",
    "                            df_presenze_null_days = df_presenze_null_days,\n",
    "                            str_area_id_presenze = str_area_id_presenze,\n",
    "                            str_presences_presenze = str_presences_presenze,\n",
    "                            col_out_presenze = col_total_presences_oct_no_hour,\n",
    "                            is_group_by_hour = False,\n",
    "                            col_hour_id = str_time_block_id_presenze,\n",
    "                            hour_id = hour_id,\n",
    "                            is_nationality_markowitz_considered = False,\n",
    "                            nationality_col = str_country_presenze,\n",
    "                            nation = nation        \n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8313475f",
   "metadata": {},
   "source": [
    "# Presences - Touristic Months"
   ]
  },
  {
   "cell_type": "code",
   "id": "d463f01e",
   "metadata": {},
   "source": [
    "df_presenze = extract_presences_vodafone_from_bucket(s3, list_files_presenze, 0)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eebc130c",
   "metadata": {},
   "source": [
    "\n",
    "# First sum over all \"COUNTRY\" entries, then take the average\n",
    "print(\"Stacking the presences data...\")\n",
    "stack_df_presenze = concat_presences(list_files_presenze, s3, col_str_day_od, col_period_id = \"PERIOD_ID\")\n",
    "list_str_days = list(stack_df_presenze[col_str_day_od].unique())\n",
    "stack_df_presenze = aggregate_presences(\n",
    "                            df_presenze = stack_df_presenze,\n",
    "                            col_str_day = col_str_day_od,\n",
    "                            str_area_id_presenze = str_area_id_presenze,\n",
    "                            str_presences_presenze = str_presences_presenze,\n",
    "                            col_out_presenze = col_total_presences_tour_no_hour,\n",
    "                            is_group_by_hour = False,\n",
    "                            col_hour_id = str_time_block_id_presenze,\n",
    "                            hour_id = hour_id,\n",
    "                            is_nationality_markowitz_considered = is_nationality_markowitz_considered,\n",
    "                            nationality_col = str_country_presenze,\n",
    "                            nation = nation\n",
    "                        )\n",
    "stack_df_presenze = aggregate_presences(df = stack_df_presenze, \n",
    "                                        list_columns_groupby = list_columns_groupby, \n",
    "                                        str_col_trips_to_be_aggregated = str_col_trips_to_be_aggregated, \n",
    "                                        str_col_name_aggregated = str_col_name_aggregated,\n",
    "                                        method_aggregation = \"sum\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44ab995d",
   "metadata": {},
   "source": [
    "#list_days_concatenated, stack_df_presenze = extract_list_days_presences(list_files_presenze, s3, col_str_day_od)\n",
    "stack_df_presenze_mean_var = compute_time_series_markowitz(\n",
    "                                                        stack_df_presenze = stack_df_presenze,\n",
    "                                                        df_presenze_null_days = df_presenze_null_days,\n",
    "                                                        str_area_id_presenze = str_area_id_presenze,\n",
    "                                                        col_total_presences_tour_no_hour = col_total_presences_tour_no_hour,\n",
    "                                                        col_total_presences_oct_no_hour = col_total_presences_oct_no_hour,\n",
    "                                                        col_tot_diff_oct = col_tot_diff_oct,\n",
    "                                                        col_tot_diff_october_mean_0 = col_tot_diff_october_mean_0,\n",
    "                                                        col_tot_diff_october_mean_0_var_1 = col_tot_diff_october_mean_0_var_1\n",
    "                                                        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "39fd05f3",
   "metadata": {},
   "source": [
    "cities_gdf = gpd.read_file(os.path.join(os.getcwd(),\"Data\",\"mavfa-fbk_AIxPA_tourism-delivery_2025.08.22-zoning\",\"fbk-aixpa-turismo.shp\"))\n",
    "cities_gdf.join(stack_df_presenze_mean_var.to_pandas().set_index(str_area_id_presenze), on=str_area_id_presenze).explore(col_tot_diff_oct, cmap=\"OrRd\")#.plot(col_tot_diff_oct, cmap=\"OrRd\",legend=True)\n",
    "#plt.savefig(\"total_diff_oct.png\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1fb502e5",
   "metadata": {},
   "source": [
    "# Compute Correlation Matrix from Time-Series"
   ]
  },
  {
   "cell_type": "code",
   "id": "39a47707",
   "metadata": {},
   "source": [
    "correlation_df = compute_correlation_matrix_df_from_time_series(stack_df_presenze_mean_var = stack_df_presenze_mean_var,\n",
    "                                                   str_area_id_presenze = str_area_id_presenze,\n",
    "                                                   col_str_day_od = col_str_day_od,\n",
    "                                                   col_tot_diff_october_mean_0_var_1 = col_tot_diff_october_mean_0_var_1,\n",
    "                                                   str_column_cov = str_column_cov)    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "460e6a03",
   "metadata": {},
   "source": [
    "# RMT Clean Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4969574",
   "metadata": {},
   "source": [
    "# Complete Markowitz Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "id": "244ddfc1",
   "metadata": {},
   "source": [
    "\n",
    "from global_import import *\n",
    "%matplotlib inline\n",
    "project_tourism = dh.get_or_create_project(\"overtourism1\")\n",
    "endpoint_url=\"http://minio:9000\"\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                    endpoint_url=endpoint_url)\n",
    "\n",
    "bucket = s3.Bucket('datalake')\n",
    "\n",
    "cities_gdf = gpd.read_file(os.path.join(os.getcwd(),\"Data\",\"mavfa-fbk_AIxPA_tourism-delivery_2025.08.22-zoning\",\"fbk-aixpa-turismo.shp\"))\n",
    "\n",
    "print(\"Extract list of files from bucket...\")\n",
    "list_files_od, list_files_presenze, list_str_dates_yyyymm = extract_filenames_and_date_from_bucket(bucket)\n",
    "date_in_file_2_skip = {'projects/tourism/_origin/vodafone-aixpa/od-mask_202407.parquet':\"2024-08-08\",\n",
    "                    'projects/tourism/_origin/vodafone-aixpa/od-mask_202408.parquet':\"2024-07-23\"}\n",
    "# NOTE: Extract the null day\n",
    "print(\"Initialize null day OD-presenze...\")\n",
    "df_presenze_null_days = extract_presences_vodafone_from_bucket(s3 = s3,\n",
    "                                                            list_files_presenze = list_files_presenze, \n",
    "                                                            i = 2)                                                                                                      # NOTE: download the file from the bucket\n",
    "df_presenze_null_days = add_is_weekday_from_period_presenze_null_days(df = df_presenze_null_days, \n",
    "                                                                      period_col= str_period_id_presenze, \n",
    "                                                                      is_weekday_col= col_str_is_week)\n",
    "\n",
    "\n",
    "# NOTE: Extract the stack of presences -> the overtouristic dataframe for presences\n",
    "print(\"Stacking the presences data...\")\n",
    "stack_df_presenze_original = concat_presences(list_files_presences = list_files_presenze, \n",
    "                                    s3 = s3, \n",
    "                                    col_str_day_od = col_str_day_od, \n",
    "                                    col_period_id = str_period_id_presenze)\n",
    "\n",
    "# NOTE: Add holiday column\n",
    "stack_df_presenze_original = add_holiday_columun_df_presenze(stack_df_presenze = stack_df_presenze_original, \n",
    "                                                    col_str_day_od = col_str_day_od,\n",
    "                                                    public_holidays = public_holidays,\n",
    "                                                    col_str_is_week = col_str_is_week)\n",
    "\n",
    "is_covariance_standardized = False\n",
    "for is_weekday in week_days:\n",
    "    cities_gdf = gpd.read_file(os.path.join(os.getcwd(),\"Data\",\"mavfa-fbk_AIxPA_tourism-delivery_2025.08.22-zoning\",\"fbk-aixpa-turismo.shp\"))\n",
    "    columns_portfolio = []\n",
    "    for hour_id in hour_ids:\n",
    "        # NOTE: Filter by weekday / holiday -> Doing every hour since compute average takes away all the columns (it is logically inconsistent)\n",
    "        stack_df_presenze_week_day = stack_df_presenze_original.filter(pl.col(col_str_is_week) == is_weekday)    \n",
    "        df_presenze_null_days_week_day = df_presenze_null_days.filter(pl.col(col_str_is_week) == is_weekday)\n",
    "        # NOTE: in aggregate_presences                                                                        # None means all nations    \n",
    "        col_total_presences_oct_no_hour = f\"total_presences_oct_no_hour_{hour_id}\"                          # total presences in October without filtering by hour\n",
    "        col_total_presences_tour_no_hour = f\"total_presences_no_hour_{hour_id}\"                             # total presences (touristic months) without filtering by hour\n",
    "        # NOTE: To insert inside loop in hours\n",
    "        col_total_presences_tour = f\"total_presences_{hour_id}\"                                             # total presences at hour_id\n",
    "        # NOTE: When starting Markowitz\n",
    "        col_tot_diff_oct = f\"total_diff_oct_{hour_id}\"                                                      # total difference October                   \n",
    "        col_tot_diff_october_mean_0 = f\"total_diff_october_mean_0_{hour_id}\"                                # total difference October - mean 0   \n",
    "        col_tot_diff_october_mean_0_var_1 = f\"total_diff_october_mean_0_var_1_{hour_id}\"                    # total difference October - mean 0 - var 1  (For random matrix standardization)\n",
    "        str_column_cov = f\"cov_{hour_id}\"\n",
    "        str_col_portfolio = f\"portfolio_{hour_id}\"\n",
    "        # NOTE: Add the colum for the portfolio associated to the hour\n",
    "        columns_portfolio.append(str_col_portfolio)\n",
    "        col_tot_diff_october_var_1 = f\"total_diff_october_var_1_{hour_id}\"\n",
    "        col_expected_return = f\"expected_return_{hour_id}\"\n",
    "        col_std = f\"std_day_{hour_id}\"\n",
    "\n",
    "\n",
    "\n",
    "        ########################################################\n",
    "        ############### NULL DAY INITIALIZATION ################\n",
    "        ########################################################\n",
    "\n",
    "        print(\"Compute the total number of presences for each AREA_ID...\")\n",
    "        df_presenze_null_days_week_day = compute_presences_average(\n",
    "                                                        df_presenze_null_days = df_presenze_null_days_week_day,\n",
    "                                                        str_area_id_presenze = str_area_id_presenze,\n",
    "                                                        str_presences_presenze = str_presences_presenze,\n",
    "                                                        col_out_presenze = col_total_presences_oct_no_hour,\n",
    "                                                        is_group_by_hour = False,\n",
    "                                                        col_hour_id = str_time_block_id_presenze,\n",
    "                                                        hour_id = hour_id,\n",
    "                                                        is_nationality_markowitz_considered = False,\n",
    "                                                        nationality_col = str_country_presenze,\n",
    "                                                        nation = nation        \n",
    "                                                        )\n",
    "\n",
    "\n",
    "        #############################################################\n",
    "        ############ PREPROCESS RAW DATA TO MARKOWITZ ###############\n",
    "        ############################################################# \n",
    "        # NOTE: Length time\n",
    "        list_str_days = list(stack_df_presenze_original[col_str_day_od].unique())\n",
    "        # NOTE: Aggregate by nation and different groups -> This defines the the dataframe that associate a count to each t in T. NOTE that without this you have more groups of presences for each day\n",
    "        stack_df_presenze_week_day = aggregate_presences(\n",
    "                                                        df_presenze = stack_df_presenze_week_day,\n",
    "                                                        col_str_day = col_str_day_od,\n",
    "                                                        str_area_id_presenze = str_area_id_presenze,\n",
    "                                                        str_presences_presenze = str_presences_presenze,\n",
    "                                                        col_out_presenze = col_total_presences_tour_no_hour,\n",
    "                                                        is_group_by_hour = True,\n",
    "                                                        col_hour_id = str_time_block_id_presenze,\n",
    "                                                        hour_id = hour_id,\n",
    "                                                        is_nationality_markowitz_considered = is_nationality_markowitz_considered,\n",
    "                                                        nationality_col = str_country_presenze,\n",
    "                                                        nation = nation\n",
    "                                                    )\n",
    "        # NOTE: Compute correlation matrix X^T X (Wishart) in df format \n",
    "        if is_covariance_standardized:\n",
    "            column_return = col_tot_diff_oct +\"_over_std\"    \n",
    "        else:\n",
    "            column_return = col_tot_diff_oct\n",
    "\n",
    "        # NOTE: Compute the normalized covariance -> It is not for testing but chooses the return from the expectation\n",
    "        stack_df_presenze = compute_starting_risk_column_from_stack_df(df_presenze_null_days = df_presenze_null_days_week_day,\n",
    "                                                    stack_df_presenze = stack_df_presenze_week_day,\n",
    "                                                    str_area_id_presenze = str_area_id_presenze,\n",
    "                                                    col_total_presences_tour_no_hour = col_total_presences_tour_no_hour,\n",
    "                                                    col_total_presences_oct_no_hour = col_total_presences_oct_no_hour,\n",
    "                                                    col_return = column_return\n",
    "                                                    )        \n",
    "        # NOTE: Compute the expected return -> It is not for testing but chooses the return from the expectation\n",
    "        df_mean = compute_expected_return_from_stack_df(stack_df_presenze = stack_df_presenze,\n",
    "                                                        col_return = column_return,                                      # NOTE: This is expected i markowitz to be: col_tot_diff_oct\n",
    "                                                        col_expected_return = col_expected_return,\n",
    "                                                        str_area_id_presenze = str_area_id_presenze,\n",
    "                                                        is_return_standardized = is_covariance_standardized,\n",
    "                                                        col_std = col_std\n",
    "                                                        )\n",
    "        # NOTE: We define here the expected return: -> other approaches could be inserted here.\n",
    "        expected_return = df_mean[col_expected_return].to_numpy()\n",
    "        # NOTE: Standardize the return time series\n",
    "        stack_df_presenze_mean_var = standardize_return_stack_df(stack_df_presenze = stack_df_presenze,\n",
    "                                                                df_mean = df_mean,\n",
    "                                                                col_return = column_return,\n",
    "                                                                str_area_id_presenze = str_area_id_presenze,\n",
    "                                                                is_standardize_return = is_covariance_standardized,\n",
    "                                                                col_std = col_std)\n",
    "\n",
    "        correlation_df = compute_correlation_matrix_df_from_time_series(stack_df_presenze_mean_var = stack_df_presenze_mean_var,\n",
    "                                                                        str_area_id_presenze = str_area_id_presenze,\n",
    "                                                                        col_str_day_od = col_str_day_od,\n",
    "                                                                        col_return = column_return,\n",
    "                                                                        str_column_cov = str_column_cov)    \n",
    "        # NOTE: Extract area_to_index and index_to_area\n",
    "        area_to_index, index_to_area = get_area_id_to_idx_mapping(cov_df = correlation_df, \n",
    "                                                                str_area_id_presenze = str_area_id_presenze)\n",
    "\n",
    "        ##############################################################\n",
    "        ####################### RMT Clean Matrix #####################\n",
    "        ##############################################################\n",
    "\n",
    "        # NOTE: Compute q = T/N\n",
    "        q = from_areas_and_times_to_q(area_to_index = area_to_index,\n",
    "                                    list_str_days = list_str_days)\n",
    "\n",
    "        \n",
    "        # NOTE: Transform covariance DataFrame into numpy matrix and create area mapping\n",
    "        cov_matrix_numpy = from_df_correlation_to_numpy_matrix(cov_df = correlation_df, \n",
    "                                                            str_area_id_presenze = str_area_id_presenze, \n",
    "                                                            str_column_cov = str_column_cov, \n",
    "                                                            area_to_index = area_to_index)\n",
    "        # NOTE: Clean the correlation matrix using RMT\n",
    "        C_clean, eigvals_clean, eigvecs = rmt_clean_correlation_matrix(C = cov_matrix_numpy, \n",
    "                                                                       q = q,\n",
    "                                                                       is_bulk_mean = True)\n",
    "\n",
    "        \n",
    "        # NOTE: Compute MP limits and mask of significant eigenvalues\n",
    "        if is_covariance_standardized:\n",
    "            sigma = None\n",
    "        else:\n",
    "            sigma = np.mean(df_mean[col_std].to_numpy())\n",
    "        lambda_minus, lambda_plus, mask_eigvals = compute_MP_limits_and_mask(eigvals_clean, \n",
    "                                                                            q, \n",
    "                                                                            is_covariance_standardized= is_covariance_standardized, \n",
    "                                                                            sigma = sigma)        \n",
    "                                                                            \n",
    "#        plot_pastur(eigvals_clean)\n",
    "\n",
    "        ##############################################################\n",
    "        #################### Markowitz procedure #####################\n",
    "        ##############################################################\n",
    "        # NOTE: Extract portfolio weights from significant eigenpairs\n",
    "        portfolio_weights = extract_portfolio_from_eigenpairs(C_clean = C_clean, \n",
    "                                                              eigvals_clean = eigvals_clean, \n",
    "                                                              eigvecs = eigvecs, \n",
    "                                                              expected_return = expected_return, \n",
    "                                                              sum_w = 1,\n",
    "                                                              is_normalize_portfolio=True)\n",
    "        # NOTE: Map portfolio weights to cities_gdf and plot -> compute the portoflio \n",
    "        cities_gdf = map_portfolio_numpy_to_cities_gdf(cities_gdf = cities_gdf,\n",
    "                                        portfolio_weights = portfolio_weights,\n",
    "                                        index_to_area = index_to_area,\n",
    "                                        str_area_id_presenze = str_area_id_presenze,\n",
    "                                        str_col_portfolio = str_col_portfolio)\n",
    "        cities_gdf = cities_gdf.merge(df_mean.to_pandas(),on=str_area_id_presenze)\n",
    "        fig,ax = plt.subplots(1,3, figsize = (10,10))\n",
    "        plot_polygons_and_with_scalar_field(cities_gdf,str_col_portfolio,ax[0],fig,title = f\"portfolio {hour_id} {is_weekday}\")        \n",
    "        plot_polygons_and_with_scalar_field(cities_gdf,col_expected_return,ax[1],fig,title = \"<oct - day> \")\n",
    "        plot_polygons_and_with_scalar_field(cities_gdf,col_std,ax[2],fig,title = \"standard deviation day\")\n",
    "        plt.show(fig)\n",
    "        plt.close(fig)\n",
    "    # NOTE: Plot portfolio map\n",
    "    path_base_portfolio = os.path.join(os.getcwd(),\"Output\",f\"{is_weekday}\")\n",
    "    path_save_portfolio = os.path.join(path_base_portfolio,f\"portfolio_map_{hour_id}.html\")\n",
    "    os.makedirs(path_base_portfolio,exist_ok=True)\n",
    "    plot_portforlio_map_multiple_layers(cities_gdf = cities_gdf,\n",
    "                                    str_area_id_presenze = str_area_id_presenze,\n",
    "                                    columns_to_plot = columns_portfolio,\n",
    "                                    str_col_comuni_name = str_col_comuni_name,\n",
    "                                    save_path = path_save_portfolio)\n",
    "    cities_gdf.to_file(os.path.join(path_base_portfolio,\"goedataframe_input_plots_markowitz.geojson\"))\n",
    "    informative_text_output = \"Explicit description variables needed for plot: \" + f\"\\nstr_area_id_presenze = {str_area_id_presenze}\\n columns_to_plot: \"\n",
    "    for col in columns_portfolio:\n",
    "        informative_text_output += col +\", \"\n",
    "    informative_text_output += f\"\\nstr_col_comuni_name: {str_col_comuni_name}\"\n",
    "    with open(os.path.join(path_base_portfolio,\"output_variable_description.txt\"), \"w\") as f:\n",
    "        f.write(informative_text_output)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f63626ed",
   "metadata": {},
   "source": [
    "# write a snippet that allows me to show three dimensional relation between str_col_portfolio, col_expected_return, col_std in cities_gdf. assume I have already computed these columns in cities_gdf. use matplotlib to create a 3d scatter plot with str_col_portfolio on the x axis, col_expected_return on the y axis and col_std on the z axis. color the points based on col_std using a colormap. add axis labels and a colorbar to indicate the mapping of colors to col_std values. show the plot. \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "sc = ax.scatter(cities_gdf[col_expected_return], cities_gdf[col_std],cities_gdf[str_col_portfolio], c=cities_gdf[str_col_portfolio], cmap='viridis')\n",
    "ax.set_xlabel(col_std)\n",
    "ax.set_ylabel(col_expected_return)\n",
    "ax.set_zlabel(str_col_portfolio)\n",
    "plt.colorbar(sc, label=str_col_portfolio)\n",
    "plt.show(fig)\n",
    "plt.close(fig)\n",
    "fig1,ax1 = plt.subplots(1,1,figsize=(6,6))\n",
    "ax1.scatter(cities_gdf[col_expected_return], cities_gdf[str_col_portfolio])\n",
    "ax1.set_xlabel(col_expected_return)\n",
    "ax1.set_ylabel(str_col_portfolio)\n",
    "plt.show(fig1)\n",
    "plt.close(fig1)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40bbd388",
   "metadata": {},
   "source": [
    "cities_gdf.merge(stack_df_presenze_week_day.to_pandas(), on = \"AREA_ID\").plot(\"total_presences_no_hour_18\", cmap=\"OrRd\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe995cc5",
   "metadata": {},
   "source": [
    "for i,eigvect in enumerate(eigvecs):\n",
    "    if eigvals_clean[i] > lambda_plus:\n",
    "        plt.hist(eigvect,label=f\"eigenvector {i}\")\n",
    "plt.legend()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3ae324fe",
   "metadata": {},
   "source": [
    "# Check Gaussianity Distribution for low eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "id": "e6a270da",
   "metadata": {},
   "source": [
    "from scipy.optimize import curve_fit\n",
    "mus = []\n",
    "sigmas = []\n",
    "for i,eigvect in enumerate(eigvecs):\n",
    "    if eigvals_clean[i] < lambda_minus:\n",
    "        n, bins = np.histogram(eigvect, bins=30, density=True)\n",
    "        try:\n",
    "            popt, pcov = curve_fit(lambda x, a, b, c : a * np.exp(-(x-b)**2/c), bins[:-1], n)\n",
    "            plt.plot(bins[:-1], popt[0] * np.exp(-(bins[:-1]-popt[1])**2/popt[2]), label=f\"fit {i}\")\n",
    "            plt.hist(eigvect, bins=30, density=True, alpha=0.5)\n",
    "            plt.legend()\n",
    "            sigmas.append(popt[2])\n",
    "            mus.append(popt[1])\n",
    "            print(f\"Eigenvector {i}: mu = {popt[1]:.4f}, sigma = {np.sqrt(popt[2]):.4f}\")\n",
    "        except:\n",
    "            pass\n",
    "fig,ax = plt.subplots(1,1,figsize=(6,6))\n",
    "ax.scatter(mus, sigmas, 'o')\n",
    "# --- IGNORE ---\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b2e046fc",
   "metadata": {},
   "source": [
    "# Markowitz from Presences"
   ]
  },
  {
   "cell_type": "code",
   "id": "32783cf5",
   "metadata": {},
   "source": [
    "target_return_markowitz = 1\n",
    "\n",
    "markowitz_portfolio(df = stack_df_presenze_mean_var, \n",
    "                    col_id = str_area_id_presenze,\n",
    "                    col_date = col_str_day_od,\n",
    "                    col_price = None,\n",
    "                    col_return = col_tot_diff_october_mean_0_var_1,\n",
    "                    col_portfolio_weight = \"portfolio_weight\",\n",
    "                    target_return = target_return_markowitz)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4402d8b1",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "q = 142/61\n",
    "lambda_plus = (1 + np.sqrt(1/q))**2\n",
    "lambda_minus = (1 - np.sqrt(1/q))**2\n",
    "\n",
    "\n",
    "def marchenko_pastur_pdf(x, q, sigma=1.0):\n",
    "    \"\"\"Marchenko-Pastur probability density function.\"\"\"\n",
    "    lambda_plus = sigma**2 * (1 + np.sqrt(1/q))**2\n",
    "    lambda_minus = sigma**2 * (1 - np.sqrt(1/q))**2\n",
    "    if lambda_minus < x < lambda_plus:\n",
    "        return (1 / (2 * np.pi * q * sigma**2 * x)) * np.sqrt((lambda_plus - x) * (x - lambda_minus))\n",
    "    else:\n",
    "        return 0\n",
    "def plot_pastur(eigvals_clean):\n",
    "    fig1,ax1 = plt.subplots(1,1,figsize=(6,6))\n",
    "    ax1.hist(eigvals_clean, bins=50, density=True, alpha=0.5, label=\"Eigenvalues Clean\", color=\"orange\")\n",
    "    #ax1.hist(np.linalg.eigvals(cov_matrix_numpy), bins=50, density=True, alpha=0.5, label=\"Eigenvalues Original\", color=\"blue\")\n",
    "    x = np.linspace(0.01, 15, 1000)\n",
    "    y = [marchenko_pastur_pdf(xi, q) for xi in x]\n",
    "    ax1.plot(x, y, label=\"Marchenko-Pastur PDF\", color=\"green\")\n",
    "    ax1.axvline(x=lambda_plus, color='red', linestyle='--', label=r'$\\lambda_{+}$')\n",
    "    ax1.axvline(x=lambda_minus, color='green', linestyle='--', label=r'$\\lambda_{-}$')\n",
    "    plt.title(\"Eigenvalues Distribution\")\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(10,10))\n",
    "#sns.heatmap(cov_matrix_numpy, ax=ax[0])\n",
    "sns.heatmap(C_clean, ax=ax[1])\n",
    "\n",
    "fig2,ax2 = plt.subplots(1,1,figsize=(6,6))\n",
    "ax2.hist(portfolio_weights, bins=20, density=True, alpha=0.5, label=\"Portfolio Weights\", color=\"blue\")\n",
    "plt.title(\"Portfolio Weights\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6b460d2a",
   "metadata": {},
   "source": [
    "len(portfolio_weights)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7c26aeea",
   "metadata": {},
   "source": [
    "# Merge GeoDf and Plot"
   ]
  },
  {
   "cell_type": "code",
   "id": "41cea799",
   "metadata": {},
   "source": [
    "cities_gdf = gpd.read_file(os.path.join(os.getcwd(),\"Data\",\"mavfa-fbk_AIxPA_tourism-delivery_2025.08.22-zoning\",\"fbk-aixpa-turismo.shp\"))\n",
    "cities_gdf\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "73b0a4c7",
   "metadata": {},
   "source": [
    "# Map portfolio weights onto cities_gdf using parameterized area id column\n",
    "# Assumes `index_to_area` is a dict[int, area_id] and `portfolio_weights` is array-like aligned by index\n",
    "# `str_area_id_presenze` must match the area identifier column in cities_gdf (e.g., \"AREA_ID\").\n",
    "# Avoid SettingWithCopyWarning by operating on an explicit copy and using .loc assignment.\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "if not isinstance(cities_gdf, gpd.GeoDataFrame):\n",
    "    raise TypeError(\"cities_gdf must be a GeoDataFrame\")\n",
    "\n",
    "# Work on a fresh copy to ensure we are not mutating a view/slice\n",
    "cities_gdf = cities_gdf.copy()\n",
    "\n",
    "# Build mapping from area_id -> weight (recompute to ensure availability)\n",
    "area_to_weight = {area: float(portfolio_weights[idx]) for idx, area in index_to_area.items()}\n",
    "\n",
    "# Assign the portfolio column by mapping the area identifier using .loc\n",
    "mapped = cities_gdf[str_area_id_presenze].map(area_to_weight)\n",
    "cities_gdf.loc[:, \"portfolio\"] = mapped\n",
    "\n",
    "# Optional: report any areas missing a weight (not present in index_to_area)\n",
    "_missing = cities_gdf[\"portfolio\"].isna()\n",
    "n_missing = int(_missing.sum())\n",
    "if n_missing:\n",
    "    print(\n",
    "        f\"Warning: {n_missing} areas in cities_gdf had no portfolio weight mapping.\",\n",
    "        \"Examples:\",\n",
    "        cities_gdf.loc[_missing, str_area_id_presenze].head().tolist(),\n",
    "    )\n",
    "    # If desired, uncomment to set missing weights to 0.0 instead of NaN\n",
    "    # cities_gdf.loc[:, \"portfolio\"] = cities_gdf[\"portfolio\"].fillna(0.0)\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,10))\n",
    "cities_gdf.plot(ax = ax, column=\"portfolio\", cmap=\"OrRd\",legend=True)\n",
    "plt.savefig(\"portfolio_map.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d7834348",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "46c6670e",
   "metadata": {},
   "source": [
    "# Plot/rendering setup for VS Code notebooks\n",
    "# - Force matplotlib inline backend\n",
    "# - Improve retina rendering\n",
    "# - Provide display() for HTML maps (folium/geopandas.explore)\n",
    "from IPython import get_ipython\n",
    "ip = get_ipython()\n",
    "if ip is not None:\n",
    "    ip.run_line_magic(\"matplotlib\", \"inline\")\n",
    "    try:\n",
    "        from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "        set_matplotlib_formats(\"retina\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d2a8d95e",
   "metadata": {},
   "source": [
    "# Explicitly display the interactive map (folium under the hood)\n",
    "_map = cities_gdf.explore(column=\"portfolio\", cmap=\"viridis\", tooltip=str_area_id_presenze)\n",
    "display(_map)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "48be85c7",
   "metadata": {},
   "source": [
    "# Show Portfolio Map"
   ]
  },
  {
   "cell_type": "code",
   "id": "f7d1dbbd",
   "metadata": {},
   "source": [
    "# Matplotlib heatmap\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cov_matrix_numpy, cmap='viridis', cbar=True, square=True, annot=False, fmt='.1f', ax=ax)\n",
    "ax.set_title('Covariance Matrix Heatmap')\n",
    "ax.set_xlabel('Area Index')\n",
    "ax.set_ylabel('Area Index')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e58d2169",
   "metadata": {},
   "source": [
    "str_col_risk_presences = f\"risk_{user_profile}_t_{str_t}_{str_t1}\"   \n",
    "str_col_return_presences = f\"return_{user_profile}_t_{str_t}_{str_t1}\"\n",
    "str_col_corr_dist_matrix = f\"corr_{user_profile}_t_{str_t}_{str_t1}\"                                                                                                  # column name for the correlation with the distance matrix\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7070aeed",
   "metadata": {},
   "source": [
    "\n",
    "# NOTE: Link together data from cities_gdf and corresponding index to df_presenze, to have the AREA_CODE column and presenze\n",
    "df_presenze_unique = df_presenze_pd.drop_duplicates(subset=[str_area_id_presenze], keep='first')\n",
    "cities_gdf = cities_gdf.merge(df_presenze_unique, left_on=str_area_id_presenze,right_on=str_area_id_presenze,how=\"left\")\n",
    "map_idx_cities_gdf_2_area_code = dict(zip(cities_gdf.index, cities_gdf[str_area_id_presenze]))  # create a map from the index of the cities gdf to the area code\n",
    "# NOTE: Add column indices from AREA_CODE that is the one characteristic of df_presenze, df_od, but not to the case of generated  by gravity flows\n",
    "df_distance_matrix = add_column_area_code_OD_df_distance(df_distance_matrix,\n",
    "                                                        map_idx_cities_gdf_2_area_code,\n",
    "                                                        str_col_origin=str_col_origin,\n",
    "                                                        str_col_destination=str_col_destination,\n",
    "                                                        str_area_code_origin_col=str_area_code_origin_col,\n",
    "                                                        str_area_code_destination_col=str_area_code_destination_col\n",
    "                                                        )  # add the area code to the origin and destination columns of the distance matrix\n",
    "# NOTE: Time intervals of interest\n",
    "list_time_intervals = [[7,8],[8,9],[10,11],[16,17],[18,19],[25,26]]                                                                                                                                # NOTE: time intervals of interest in hours   \n",
    "# NOTE: Time intervals\n",
    "for time_interval in list_time_intervals:\n",
    "    print(\"Time interval: \", time_interval)                                                                                                                               # for each time interval\n",
    "    int_hour_start_window_interest = time_interval[0]                                                                                                                   # start time window of interest\n",
    "    int_hour_end_window_interest = time_interval[1]                                                                                                                     # end time window of interest\n",
    "    int_min_aggregation_OD = 60                                                                                                                                         # aggregation time in minutes\n",
    "    # NOTE: Profile user\n",
    "    # NOTE: This is used to filter the buses\n",
    "    if int_hour_start_window_interest != 25:                                                                                                                   # for each user profile                                   \n",
    "        # NOTE: chnge the time according to the source                                                                                                                  \n",
    "        time_vector_OD = pd.timedelta_range(start = f\"{int_hour_start_window_interest}h\",\n",
    "                                            end = f\"{int_hour_end_window_interest}h\",\n",
    "                                            freq = f\"{int_min_aggregation_OD}min\")\n",
    "        str_time_vector = np.linspace(int_hour_start_window_interest,int_hour_end_window_interest,len(time_vector_OD))                                                  # time vector in hours\n",
    "        is_fluxes_hourly = True                                                                                                                         # NOTE: if the fluxes are generated hourly\n",
    "    else:\n",
    "        is_fluxes_hourly = False                                                                                                                         # NOTE: if the fluxes are generated hourly\n",
    "    print(f\"User profile: {user_profile} - Time vector: {time_vector_OD}\")                                                                                          # print the user profile and the time vector\n",
    "    for t_idx in range(len(time_vector_OD)-1):                                                                                                                      # NOTE: t_start_OD is a list of time in HH:MM:SS format\n",
    "        # Initialize variables that are relevant for the time analysis (time and user profile)\n",
    "        str_t = str_time_vector[t_idx]                                                                                                                              # time in HH:MM:SS format\n",
    "        str_t1 = str_time_vector[t_idx + 1]                                                                                                                         # time in HH:MM:SS format\n",
    "        t_i = time_vector_OD[t_idx]                                                                                                                                 # time in hours\n",
    "        t_i1 = time_vector_OD[t_idx+1]                                                                                                                              # time in hours\n",
    "        print(f\"Time: {str_t} - {str_t1}\")                                                                                                                          # print the time\n",
    "        # Parameters - df_presences\n",
    "        str_col_risk_presences = f\"risk_{user_profile}_t_{str_t}_{str_t1}\"   \n",
    "        str_col_return_presences = f\"return_{user_profile}_t_{str_t}_{str_t1}\"\n",
    "        str_col_corr_dist_matrix = f\"corr_{user_profile}_t_{str_t}_{str_t1}\"                                                                                                  # column name for the correlation with the distance matrix\n",
    "        str_dir_output_date = os.path.join(config[str_dir_output],\"Diffusion 3\")                                                                      # NOTE: create a directory for the date\n",
    "        Path(str_dir_output_date).mkdir(parents=True, exist_ok=True)                                                                                                # create the directory if it does not exist                \n",
    "        # TODO:                                                                                 # extract the day from the file name\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ccc0c592",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
