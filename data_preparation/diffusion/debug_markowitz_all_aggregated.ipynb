{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bb688d3",
   "metadata": {},
   "source": [
    "# Script to compute Markowitz portfolio aggregated by all cases"
   ]
  },
  {
   "cell_type": "code",
   "id": "965051b4",
   "metadata": {},
   "source": [
    "\n",
    "from scripts_overtoruism_analysis.Markowitz_pipeline import compute_filters_and_messages_for_case_analysis_diffusione_3\n",
    "from global_import import *\n",
    "%matplotlib inline\n",
    "project_tourism = dh.get_or_create_project(\"overtourism1\")\n",
    "endpoint_url=\"http://minio:9000\"\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                    endpoint_url=endpoint_url)\n",
    "\n",
    "bucket = s3.Bucket('datalake')\n",
    "\n",
    "cities_gdf = gpd.read_file(os.path.join(os.getcwd(),\"Data\",\"mavfa-fbk_AIxPA_tourism-delivery_2025.08.22-zoning\",\"fbk-aixpa-turismo.shp\"))\n",
    "\n",
    "print(\"Extract list of files from bucket...\")\n",
    "list_files_od, list_files_presenze, list_str_dates_yyyymm = extract_filenames_and_date_from_bucket(bucket)\n",
    "date_in_file_2_skip = {'projects/tourism/_origin/vodafone-aixpa/od-mask_202407.parquet':\"2024-08-08\",\n",
    "                    'projects/tourism/_origin/vodafone-aixpa/od-mask_202408.parquet':\"2024-07-23\"}\n",
    "# NOTE: Extract the null day\n",
    "print(\"Initialize null day OD-presenze...\")\n",
    "df_presenze_null_days = extract_presences_vodafone_from_bucket(s3 = s3,\n",
    "                                                            list_files_presenze = list_files_presenze, \n",
    "                                                            i = 2)                                                                                                      # NOTE: download the file from the bucket\n",
    "df_presenze_null_days = add_is_weekday_from_period_presenze_null_days(df = df_presenze_null_days, \n",
    "                                                                      period_col= str_period_id_presenze, \n",
    "                                                                      is_weekday_col= col_str_is_week)\n",
    "\n",
    "# NOTE: Extract the stack of presences -> the overtouristic dataframe for presences\n",
    "print(\"Stacking the presences data...\")\n",
    "stack_df_presenze_original = concat_presences(list_files_presences = list_files_presenze, \n",
    "                                    s3 = s3, \n",
    "                                    col_str_day_od = col_str_day_od, \n",
    "                                    col_period_id = str_period_id_presenze)\n",
    "\n",
    "# NOTE: Add holiday column\n",
    "stack_df_presenze_original = add_holiday_columun_df_presenze(stack_df_presenze = stack_df_presenze_original, \n",
    "                                                    col_str_day_od = col_str_day_od,\n",
    "                                                    public_holidays = public_holidays,\n",
    "                                                    col_str_is_week = col_str_is_week)\n",
    "\n",
    "list_all_hours = list(stack_df_presenze_original[str_time_block_id_presenze].unique())\n",
    "list_all_visitors = list(stack_df_presenze_original[str_visitor_class_id_presenze].unique())\n",
    "list_all_countries = list(stack_df_presenze_original[str_country_presenze].unique())\n",
    "list_all_weekdays = list(stack_df_presenze_original[col_str_is_week].unique())\n",
    "print(f\"List all hours: {list_all_hours}, list all visitors: {list_all_visitors}, list all countries: {list_all_countries}, list all weekdays: {list_all_weekdays}\")\n",
    "case2column_names_diffusione_3 = init_dict_case2column_names_diffusione_3(AGGREGATION_NAMES_2_COLUMNS_AGGREGATION_PRESENCES)\n",
    "\n",
    "for case_pipeline in case2column_names_diffusione_3.keys():\n",
    "    print(f\"Initialize dict presences output for case_pipeline {case_pipeline}...\")\n",
    "    \n",
    "    dict_presences_output, case_2iterable_values = initialize_dict_presences_output_diffusione_3(\n",
    "        dict_case2column_names = case2column_names_diffusione_3,\n",
    "        list_all_hours = list_all_hours,\n",
    "        list_all_visitors = list_all_visitors,\n",
    "        list_all_countries = list_all_countries,\n",
    "        list_all_weekdays = list_all_weekdays,\n",
    "        case_pipeline = case_pipeline,\n",
    "    )\n",
    "\n",
    "    # Extract the iterables for this case\n",
    "    dict_iterable_name_2_iterable_values = case_2iterable_values[case_pipeline]\n",
    "    \n",
    "    # Unfold all possible combinations of present iterables\n",
    "    iterable_names = list(dict_iterable_name_2_iterable_values.keys())\n",
    "    iterable_values = [dict_iterable_name_2_iterable_values[k] for k in iterable_names]\n",
    "\n",
    "    print(f\"\\tIterating over {iterable_names}\")\n",
    "\n",
    "    for combo in product(*iterable_values):\n",
    "        combo_dict = dict(zip(iterable_names, combo))\n",
    "\n",
    "        # Extract values (or defaults if not in this case)\n",
    "        time = combo_dict.get(\"time\", None)\n",
    "        visitor = combo_dict.get(\"visitor\", None)\n",
    "        country = combo_dict.get(\"country\", None)\n",
    "        is_weekday = combo_dict.get(\"weekday\", None)\n",
    "\n",
    "        print(f\"\\tCombination: {combo_dict}\")\n",
    "\n",
    "        dict_presences_output = nested_set_dict_column_names_presences_analysis(\n",
    "            case_2iterable_values = case_2iterable_values,\n",
    "            dict_presences_output = dict_presences_output,\n",
    "            time = time,\n",
    "            visitor = visitor,\n",
    "            country = country,\n",
    "            is_weekday = is_weekday,\n",
    "            case_pipeline = case_pipeline,\n",
    "        )\n",
    "        # NOTE: Get the name of the columns for presences\n",
    "        col_aggregated_presences = nested_get_output_from_key_presences_analysis(case_2iterable_values = case_2iterable_values,dict_presences_output = dict_presences_output,\n",
    "                                                                                name_key =\"column_presences_no_baseline\",case_pipeline = case_pipeline)\n",
    "        col_aggregated_presences_baseline = nested_get_output_from_key_presences_analysis(case_2iterable_values = case_2iterable_values,dict_presences_output = dict_presences_output,\n",
    "                                                                                name_key =\"column_presences_baseline\",case_pipeline = case_pipeline)\n",
    "        # NOTE: Aggregate at the same level of aggregation null day and touristic presenze \n",
    "        stack_df_presenze_original_week_day_base = aggregate_presences_new_version(df = stack_df_presenze_original, \n",
    "                                                                    list_columns_groupby = case2column_names_diffusione_3[case_pipeline] + [col_str_day_od],\n",
    "                                                                    str_col_trips_to_be_aggregated = \"PRESENCES\",\n",
    "                                                                    str_col_name_aggregated = col_aggregated_presences,\n",
    "                                                                    method_aggregation = \"sum\")\n",
    "\n",
    "        df_presenze_null_days_week_day_base = aggregate_presences_new_version(df = df_presenze_null_days, \n",
    "                                                                list_columns_groupby = case2column_names_diffusione_3[case_pipeline],\n",
    "                                                                str_col_trips_to_be_aggregated = \"PRESENCES\",\n",
    "                                                                str_col_name_aggregated = col_aggregated_presences_baseline,\n",
    "                                                                method_aggregation = \"sum\")\n",
    "        # NOTE: Average over all days.\n",
    "        df_presenze_null_days_week_day_base = df_presenze_null_days_week_day_base.with_columns((pl.col(col_aggregated_presences_baseline)/30).floor())\n",
    "\n",
    "\n",
    "        is_covariance_standardized = True\n",
    "        cities_gdf = gpd.read_file(os.path.join(os.getcwd(),\"Data\",\"mavfa-fbk_AIxPA_tourism-delivery_2025.08.22-zoning\",\"fbk-aixpa-turismo.shp\"))\n",
    "        dict_case_2_tuple_filters, dict_case_message = compute_filters_and_messages_for_case_analysis_diffusione_3(\n",
    "                                                                    dict_case2column_names = case2column_names_diffusione_3,\n",
    "                                                                    time_of_interest = time,\n",
    "                                                                    visitor_of_interest = visitor,\n",
    "                                                                    country_of_interest = country,\n",
    "                                                                    is_weekday = is_weekday,\n",
    "                                                                )\n",
    "        # NOTE: Filter by weekday / holiday -> Doing every hour since compute average takes away all the columns (it is logically inconsistent)\n",
    "        stack_df_presenze_week_day = stack_df_presenze_original_week_day_base.filter(dict_case_2_tuple_filters[case_pipeline])    \n",
    "        df_presenze_null_days_week_day = df_presenze_null_days_week_day_base.filter(dict_case_2_tuple_filters[case_pipeline])\n",
    "\n",
    "\n",
    "\n",
    "        ########################################################\n",
    "        ############### NULL DAY INITIALIZATION ################\n",
    "        ########################################################\n",
    "\n",
    "        print(\"Compute the total number of presences for each AREA_ID...\")\n",
    "\n",
    "\n",
    "        #############################################################\n",
    "        ############ PREPROCESS RAW DATA TO MARKOWITZ ###############\n",
    "        ############################################################# \n",
    "        # NOTE: Set the columns name for the analysis\n",
    "        column_return = nested_get_output_from_key_presences_analysis(case_2iterable_values = case_2iterable_values,dict_presences_output = dict_presences_output,\n",
    "                                                                                name_key =\"column_portfolio\",case_pipeline = case_pipeline)\n",
    "        col_expected_return = nested_get_output_from_key_presences_analysis(case_2iterable_values = case_2iterable_values,dict_presences_output = dict_presences_output,\n",
    "                                                                                name_key =\"column_expected_return\",case_pipeline = case_pipeline)\n",
    "        col_std = nested_get_output_from_key_presences_analysis(case_2iterable_values = case_2iterable_values,dict_presences_output = dict_presences_output,\n",
    "                                                                name_key =\"column_std\",case_pipeline = case_pipeline)\n",
    "        str_column_cov = nested_get_output_from_key_presences_analysis(case_2iterable_values = case_2iterable_values,dict_presences_output = dict_presences_output,\n",
    "                                                                        name_key =\"column_cov\",case_pipeline = case_pipeline)\n",
    "        \n",
    "        str_col_portfolio = nested_get_output_from_key_presences_analysis(case_2iterable_values = case_2iterable_values,dict_presences_output = dict_presences_output,\n",
    "                                                                        name_key =\"column_portfolio\",case_pipeline = case_pipeline)\n",
    "        # NOTE: Extract the list of days\n",
    "        list_str_days = list(stack_df_presenze_original[col_str_day_od].unique())\n",
    "        # NOTE: Compute the normalized covariance -> It is not for testing but chooses the return from the expectation\n",
    "        stack_df_presenze = compute_starting_risk_column_from_stack_df(df_presenze_null_days = df_presenze_null_days_week_day,\n",
    "                                                    stack_df_presenze = stack_df_presenze_week_day,\n",
    "                                                    str_area_id_presenze = str_area_id_presenze,\n",
    "                                                    col_total_presences_tour_no_hour = col_aggregated_presences,\n",
    "                                                    col_total_presences_oct_no_hour =col_aggregated_presences_baseline,\n",
    "                                                    col_return = column_return\n",
    "                                                    )        \n",
    "        # NOTE: Compute the expected return -> It is not for testing but chooses the return from the expectation\n",
    "        df_mean = compute_expected_return_from_stack_df(stack_df_presenze = stack_df_presenze,\n",
    "                                                        col_return = column_return,                                      # NOTE: This is expected i markowitz to be: col_tot_diff_oct\n",
    "                                                        col_expected_return = col_expected_return,\n",
    "                                                        str_area_id_presenze = str_area_id_presenze,\n",
    "                                                        is_return_standardized = is_covariance_standardized,\n",
    "                                                        col_std = col_std\n",
    "                                                        )\n",
    "        print(f\"stack_df_presenze: {stack_df_presenze.head()}, df_mean: {df_mean.head()}, \")\n",
    "        # NOTE: We define here the expected return: -> other approaches could be inserted here.\n",
    "        expected_return = df_mean[col_expected_return].to_numpy()\n",
    "        # NOTE: Standardize the return time series\n",
    "        stack_df_presenze_mean_var = standardize_return_stack_df(stack_df_presenze = stack_df_presenze,\n",
    "                                                                df_mean = df_mean,\n",
    "                                                                col_return = column_return,\n",
    "                                                                str_area_id_presenze = str_area_id_presenze,\n",
    "                                                                is_standardize_return = is_covariance_standardized,\n",
    "                                                                col_std = col_std)\n",
    "\n",
    "        correlation_df = compute_correlation_matrix_df_from_time_series(stack_df_presenze_mean_var = stack_df_presenze_mean_var,\n",
    "                                                                        str_area_id_presenze = str_area_id_presenze,\n",
    "                                                                        col_str_day_od = col_str_day_od,\n",
    "                                                                        col_return = column_return,\n",
    "                                                                        str_column_cov = str_column_cov)    \n",
    "        # NOTE: Extract area_to_index and index_to_area\n",
    "        area_to_index, index_to_area = get_area_id_to_idx_mapping(cov_df = correlation_df, \n",
    "                                                                str_area_id_presenze = str_area_id_presenze)\n",
    "\n",
    "        ##############################################################\n",
    "        ####################### RMT Clean Matrix #####################\n",
    "        ##############################################################\n",
    "\n",
    "        # NOTE: Compute q = T/N\n",
    "        q = from_areas_and_times_to_q(area_to_index = area_to_index,\n",
    "                                    list_str_days = list_str_days)\n",
    "\n",
    "        \n",
    "        # NOTE: Transform covariance DataFrame into numpy matrix and create area mapping\n",
    "        cov_matrix_numpy = from_df_correlation_to_numpy_matrix(cov_df = correlation_df, \n",
    "                                                            str_area_id_presenze = str_area_id_presenze, \n",
    "                                                            str_column_cov = str_column_cov, \n",
    "                                                            area_to_index = area_to_index)\n",
    "        # NOTE: Clean the correlation matrix using RMT\n",
    "        C_clean, eigvals_clean, eigvecs = rmt_clean_correlation_matrix(C = cov_matrix_numpy, \n",
    "                                                                        q = q,\n",
    "                                                                        is_bulk_mean = True)\n",
    "\n",
    "        \n",
    "        # NOTE: Compute MP limits and mask of significant eigenvalues\n",
    "        if is_covariance_standardized:\n",
    "            sigma = None\n",
    "        else:\n",
    "            sigma = np.mean(df_mean[col_std].to_numpy())\n",
    "        lambda_minus, lambda_plus, mask_eigvals = compute_MP_limits_and_mask(eigvals_clean, \n",
    "                                                                            q, \n",
    "                                                                            is_covariance_standardized= is_covariance_standardized, \n",
    "                                                                            sigma = sigma)        \n",
    "                                                                            \n",
    "    #        plot_pastur(eigvals_clean)\n",
    "\n",
    "        ##############################################################\n",
    "        #################### Markowitz procedure #####################\n",
    "        ##############################################################\n",
    "        # NOTE: Extract portfolio weights from significant eigenpairs\n",
    "        portfolio_weights = extract_portfolio_from_eigenpairs(C_clean = C_clean, \n",
    "                                                                eigvals_clean = eigvals_clean, \n",
    "                                                                eigvecs = eigvecs, \n",
    "                                                                expected_return = expected_return, \n",
    "                                                                sum_w = 1,\n",
    "                                                                is_normalize_portfolio=True)\n",
    "        # NOTE: Map portfolio weights to cities_gdf and plot -> compute the portoflio \n",
    "        cities_gdf = map_portfolio_numpy_to_cities_gdf(cities_gdf = cities_gdf,\n",
    "                                        portfolio_weights = portfolio_weights,\n",
    "                                        index_to_area = index_to_area,\n",
    "                                        str_area_id_presenze = str_area_id_presenze,\n",
    "                                        str_col_portfolio = str_col_portfolio)\n",
    "        cities_gdf = cities_gdf.merge(df_mean.to_pandas(),on=str_area_id_presenze)\n",
    "        path_output_base = os.path.join(os.getcwd(),\"Output\")\n",
    "        path_base_portfolio = path_output_base \n",
    "        for branch in dict_presences_output.keys():\n",
    "            path_base_portfolio = os.path.join(path_base_portfolio,f\"{branch}\")\n",
    "            os.makedirs(path_base_portfolio,exist_ok=True)\n",
    "\n",
    "    #    fig,ax = plt.subplots(1,3, figsize = (10,10))\n",
    "        fig,ax = plt.subplots(1,1, figsize = (10,10))\n",
    "        plot_polygons_and_with_scalar_field(cities_gdf,str_col_portfolio,ax,fig,title = f\"portfolio {case_pipeline}\")        \n",
    "    #    plot_polygons_and_with_scalar_field(cities_gdf,col_expected_return,ax[1],fig,title = \"<oct - day> \")\n",
    "    #    plot_polygons_and_with_scalar_field(cities_gdf,col_std,ax[2],fig,title = \"standard deviation day\")\n",
    "        plt.show(fig)\n",
    "        plt.savefig(os.path.join(path_base_portfolio,f\"{case_pipeline}_portfolio_map.png\"),dpi =200)\n",
    "        plt.close(fig)\n",
    "        fig_pst,ax_pst = plot_pastur(eigvals_clean, q)\n",
    "        plt.savefig(os.path.join(path_base_portfolio,f\"{case_pipeline}_pastur_distribution_eigenvalues.png\"),dpi =200)\n",
    "        # NOTE: Plot portfolio map\n",
    "        path_save_portfolio = os.path.join(path_base_portfolio,f\"portfolio_map_all_aggregated_{case_pipeline}.html\")\n",
    "        os.makedirs(path_base_portfolio,exist_ok=True)\n",
    "        cities_gdf.to_file(os.path.join(path_base_portfolio,f\"geodataframe_input_plots_markowitz_{case_pipeline}.geojson\"))\n",
    "        gc.collect()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2c5b3fa9",
   "metadata": {},
   "source": [
    "# Single Trajectory -> understanding single evolution"
   ]
  },
  {
   "cell_type": "code",
   "id": "e65037f8",
   "metadata": {},
   "source": [
    "from typing import List\n",
    "class Trajectory:\n",
    "    def __init__(self, df: pl.DataFrame, id_col: str, time_col: str, list_feature_cols: List[str]):\n",
    "        self.df = df\n",
    "        self.id_col = id_col\n",
    "        self.time_col = time_col\n",
    "        self.list_feature_cols = list_feature_cols\n",
    "\n",
    "    def __get__(self):\n",
    "        return self\n",
    "    \n",
    "    def _get_ids(self):\n",
    "        return self.df[self.id_col].unique().to_list()\n",
    "    \n",
    "\n",
    "    def get_trajectory(self, id_value):\n",
    "        self.t = self.df.filter(pl.col(self.id_col) == id_value).sort(pl.col(self.time_col))[self.time_col].to_numpy()\n",
    "        if len(self.list_feature_cols) == 1:\n",
    "            self.x = self.df.filter(pl.col(self.id_col) == id_value).sort(pl.col(self.time_col))[self.list_feature_cols[0]].to_numpy()\n",
    "        else:\n",
    "            self.x = self.df.filter(pl.col(self.id_col) == id_value).sort(pl.col(self.time_col))[self.list_feature_cols].to_numpy()\n",
    "\n",
    "        return self.t, self.x\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(1,1, figsize = (10,10))\n",
    "trajs = Trajectory(df = stack_df_presenze, id_col = str_area_id_presenze, time_col = col_str_day_od, list_feature_cols = [column_return])\n",
    "list_ids = trajs._get_ids()\n",
    "for id_value in list_ids:\n",
    "    if len(trajs.list_feature_cols) == 1:\n",
    "        ax.plot(trajs.get_trajectory(id_value = id_value)[0], trajs.get_trajectory(id_value = id_value)[1], label = id_value)\n",
    "    else:\n",
    "        ax.plot(trajs.get_trajectory(id_value = id_value)[0], trajs.get_trajectory(id_value = id_value)[1], label = id_value)\n",
    "    ax.set_xticklabels(labels= trajs.get_trajectory(id_value = id_value)[0],rotation=90)\n",
    "\n",
    "ax.legend()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b13decba",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "id": "7608db55",
   "metadata": {},
   "source": [
    "import geopandas as gpd\n",
    "from Plots import plot_polygons_and_with_scalar_field\n",
    "import matplotlib.pyplot as plt\n",
    "gdf = gpd.read_file(\"/home/aamaduzzi/aixpa-overtourism-backend/scripts_overtoruism_analysis/Output/Prefestivo/goedataframe_input_plots_markowitz_all_aggregated.geojson\")\n",
    "fig,ax = plt.subplots(1,1, figsize = (10,10))\n",
    "gdf[\"people_to_move\"] = gdf.apply(lambda row: row[\"expected_return\"]*row[\"portfolio\"]*100*24, axis=1)\n",
    "plot_polygons_and_with_scalar_field(gdf,\"people_to_move\",ax,fig,title = f\"People to move Prefestivo\")        \n",
    "plt.show(fig)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f1f7d5f4",
   "metadata": {},
   "source": [
    "gdf = gpd.read_file(\"/home/aamaduzzi/aixpa-overtourism-backend/scripts_overtoruism_analysis/Output/Festivo/goedataframe_input_plots_markowitz_all_aggregated.geojson\")\n",
    "fig,ax = plt.subplots(1,1, figsize = (10,10))\n",
    "gdf[\"people_to_move\"] = gdf.apply(lambda row: row[\"expected_return\"]*row[\"portfolio\"]*100*24, axis=1)\n",
    "plot_polygons_and_with_scalar_field(gdf,\"people_to_move\",ax,fig,title = f\"People to move Festivo\")        \n",
    "plt.show(fig)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dea8488f",
   "metadata": {},
   "source": [
    "# compute fit of the expected return as exponential and plot it\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "def exponential_fit(x, a, b):\n",
    "    return a * np.exp(b * x)\n",
    "hist, bin_edges = np.histogram(gdf[\"expected_return\"], bins=30, density=True)\n",
    "bin_centers = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
    "popt, pcov = curve_fit(exponential_fit, bin_centers, hist, p0=[1, 1])\n",
    "x_fit = np.linspace(min(gdf[\"expected_return\"]), max(gdf[\"expected_return\"]), 100)\n",
    "y_fit = exponential_fit(x_fit, *popt)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(gdf[\"expected_return\"], bins=30, density=True, alpha=0.6, label='Data Histogram')\n",
    "plt.plot(x_fit, y_fit, 'r-', label=f'Exponential Fit ${popt[0]:.2f}e^{{{popt[1]:.2f}x}}$')\n",
    "plt.xlabel('Expected Return')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Histogram of Expected Return with Exponential Fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64accd79",
   "metadata": {},
   "source": [
    "gdf = gpd.read_file(\"/home/aamaduzzi/aixpa-overtourism-backend/scripts_overtoruism_analysis/Output/Feriale/goedataframe_input_plots_markowitz_all_aggregated.geojson\")\n",
    "fig,ax = plt.subplots(1,1, figsize = (10,10))\n",
    "gdf[\"people_to_move\"] = gdf.apply(lambda row: row[\"expected_return\"]*row[\"portfolio\"]*100*24, axis=1)\n",
    "plot_polygons_and_with_scalar_field(gdf,\"people_to_move\",ax,fig,title = f\"People to move Feriale\")        \n",
    "plt.show(fig)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "da9b50fe",
   "metadata": {},
   "source": [
    "# RandomTree degree 5 to predict the line"
   ]
  },
  {
   "cell_type": "code",
   "id": "8bed9d31",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "error_rf = []\n",
    "error_gb = []\n",
    "fracion = 0.1\n",
    "ns = np.arange(10, int(3010), 50)\n",
    "for n in ns:\n",
    "    t = np.arange(n)\n",
    "    y = np.sin(0.02 * t) + 0.3 * np.random.randn(n)\n",
    "\n",
    "    # Construct lag features\n",
    "    p = int(fracion * n)\n",
    "    X = np.column_stack([y[i:n - p + i] for i in range(p)])\n",
    "    y_target = y[p:]\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_target, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # --- Random Forest ---\n",
    "    rf = RandomForestRegressor(n_estimators=200, max_depth=5, random_state=0)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "    # --- Gradient Boosting ---\n",
    "    gb = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=3, random_state=0)\n",
    "    gb.fit(X_train, y_train)\n",
    "    y_pred_gb = gb.predict(X_test)\n",
    "    error_rf.append(mean_squared_error(y_test, y_pred_rf))\n",
    "    error_gb.append(mean_squared_error(y_test, y_pred_gb))\n",
    "    # Compare errors\n",
    "    print(\"RF RMSE:\", mean_squared_error(y_test, y_pred_rf))\n",
    "    print(\"GB RMSE:\", mean_squared_error(y_test, y_pred_gb))\n",
    "\n",
    "plt.plot(ns, error_rf, label='Random Forest', color='blue')\n",
    "plt.plot(ns, error_gb, label='Gradient Boosting', color='orange')\n",
    "plt.xlabel('Number of samples')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Model Performance vs. Number of Samples')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "976bc92e",
   "metadata": {},
   "source": [
    "y_rf = np.concatenate([y_train, y_pred_rf])\n",
    "y_gb = np.concatenate([y_train, y_pred_gb])\n",
    "\n",
    "plt.plot(t[p:], y_gb, label='Predicted GB', color='orange')\n",
    "plt.plot(t, y, label='True',color = 'blue',alpha=0.1)\n",
    "plt.plot(t[p:], y_rf, label='Predicted RF', color='green',alpha =0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a63d2c47",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
