{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0de7a27",
   "metadata": {},
   "source": [
    "# Analysis of Fluxes Applied on Vodafone Geometries"
   ]
  },
  {
   "cell_type": "code",
   "id": "94ef6bb2",
   "metadata": {},
   "source": [
    "from global_import import *"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "edebb6fa",
   "metadata": {},
   "source": [
    "## Platform Initialization"
   ]
  },
  {
   "cell_type": "code",
   "id": "e81fb765",
   "metadata": {},
   "source": [
    "project_tourism = dh.get_or_create_project(\"overtourism1\")\n",
    "endpoint_url=\"http://minio:9000\"\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                    endpoint_url=endpoint_url)\n",
    "\n",
    "bucket = s3.Bucket('datalake')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5b1a0bc4",
   "metadata": {},
   "source": [
    "## Set Configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "3c949dda",
   "metadata": {},
   "source": [
    "\n",
    "config = set_config(str_name_project,                       # NAME PROJECT\n",
    "               str_dir_data_path,                           # PATH DATA -> contains input data (shapefile, gtfs_data, istat_data)       \n",
    "               str_dir_output_path,                         # PATH OUTPUT -> contains output data (plots, processed data) is the base path that is branched under each condition imposed by the analysis    \n",
    "               complete_path_Istat_population,              # PATH ISTAT DATA\n",
    "               str_prefix_complete_path,                    # PREFIX COMPLETE PATH   \n",
    "               str_start_time_window_interest,              # TIME (str): START WINDOW INTEREST\n",
    "               str_end_time_window_interest,                # TIME (str): END WINDOW INTEREST\n",
    "               int_number_people_per_bus,                   # NUMBER PEOPLE PER BUS\n",
    "               str_name_file_gtfs_zip,                      # NAME FILE GTFS ZIP: google_transit_extraurbano.zip\n",
    "               str_name_dataset_gtfs,                       # NAME DATASET GTFS\n",
    "               str_name_gdf_transport,                      # NAME GDF TRANSPORT (Road Network)\n",
    "               str_name_graph_transport,                    # NAME GRAPH TRANSPORT (Graph of Road Network)\n",
    "               str_name_grid,                         # NAME GRID\n",
    "               str_name_grid_2_city,\n",
    "               str_name_shape_city,\n",
    "               str_name_centroid_city,\n",
    "               str_route_idx,\n",
    "               str_trip_idx,\n",
    "               str_stop_idx,\n",
    "               str_transport_idx,\n",
    "               str_grid_idx,\n",
    "               str_name_stop_2_trip,\n",
    "               str_name_stop_2_route,\n",
    "               str_name_grid_2_stop,\n",
    "               str_name_grid_2_route,\n",
    "               str_name_graph_2_route,\n",
    "               str_name_route_2_graph,\n",
    "               str_dir_plots_path,\n",
    "               int_hour_start_window_interest,\n",
    "               int_hour_end_window_interest,\n",
    "               int_min_aggregation_OD,\n",
    "               Lx,\n",
    "               Ly)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "622e0c00",
   "metadata": {},
   "source": [
    "# Gtfs Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "7ced8d83",
   "metadata": {},
   "source": [
    "if os.path.exists(config[f\"{str_prefix_complete_path}_{str_name_dataset_gtfs}\"]):                                             # NOTE: check if the gtfs file exists\n",
    "    print(\"GTFS file exists, loading it\")                                                                                     # NOTE: load gtfs file\n",
    "    feed = Preprocessing_gtfs(config[f\"{str_prefix_complete_path}_{str_name_dataset_gtfs}\"])                                  # NOTE: load gtfs file from the directory set -> feed object is typical of gtfs_kit\n",
    "    is_gtfs_available = False\n",
    "else:\n",
    "    print(\"GTFS file does not exist, downloading it\")                                                                         # NOTE: download gtfs file\n",
    "    is_gtfs_available = False\n",
    "    feed = None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "311581c8",
   "metadata": {},
   "source": [
    "# Comuni -> Shape file + Istat (Gravity Model)"
   ]
  },
  {
   "cell_type": "code",
   "id": "0f8c5d34",
   "metadata": {},
   "source": [
    "# ------------------  Extract Cities of interest from Vodafone ------------------ #                                                                                 # NOTE: here you can define what is the list of comuni you want to consider\n",
    "print(\"Compute comuni polygons...\")\n",
    "\n",
    "#attendences_foreigners = data_handler.vodafone_attendences_df.join(data_handler.vodafone_aree_df, on='locId', how='left',coalesce=True)                                                 # joining dataframes (just to have the needed columns)\n",
    "#names_zones_to_consider = attendences_foreigners.filter(pl.col(\"locName\").is_in([\"comune\"])).unique(\"locDescr\")[\"locDescr\"].to_list()                               # pick the comune\n",
    "complete_path_shape_gdf = config[f\"{str_prefix_complete_path}_{str_name_shape_city}\"]                                                                               # path to the shape file of the cities\n",
    "complete_path_centroid_gdf = config[f\"{str_prefix_complete_path}_{str_name_centroid_city}\"]                                                                         # path to the centroids associated to the cities\n",
    "#centroids_gdf,cities_gdf = pipeline_extract_boundary_and_centroid_gdf_from_name_comuni(names_zones_to_consider,                                                     # list of the comuni to consider        \n",
    "#                                                                                       complete_path_shape_gdf,                                                     #\n",
    "#                                                                                       complete_path_centroid_gdf)\n",
    "\n",
    "# ---------------- Extrct Population From Istat ------------------ #\n",
    "print(\"Upload Istat data...\")\n",
    "Istat_obj = Istat_population_data(complete_path_Istat_population)\n",
    "\n",
    "# NOTE: Read geometries from the shapefile of the cities\n",
    "cities_gdf = gpd.read_file(os.path.join(os.getcwd(),\"Data\",\"mavfa-fbk_AIxPA_tourism-delivery_2025.08.22-zoning\",\"fbk-aixpa-turismo.shp\"))\n",
    "#cities_gdf = gpd.read_file(os.path.join(os.getcwd(),\"Data\",\"shapefile_fbk_2025.05.21\",\"fbk.shp\"))\n",
    "\n",
    "# NOTE: Join the informations of the population (from Istat) with the geometries of the city\n",
    "cities_gdf = simple_join_cities_with_population(cities_gdf, \n",
    "                                    Istat_obj.population_by_comune,\n",
    "                                    str_col_comuni_istat = str_col_comuni_istat,\n",
    "                                    str_col_popolazione_totale = str_population_col_grid,\n",
    "                                    str_col_city_name = str_col_comuni_name,\n",
    "                                    is_Vodafone_Trento_ZDT = True)                                                                                        # NOTE: here we join the cities with the population data from Istat\n",
    "# NOTE: When the city appears in subdivisions (like Trento, Rovereto, etc.) we need to know how much of the population is in each diivision\n",
    "\n",
    "cities_gdf = add_column_area_and_fraction(cities_gdf, \n",
    "                                str_col_comuni_name,\n",
    "                                str_col_area = str_col_area,\n",
    "                                str_col_tot_area = str_col_tot_area,\n",
    "                                str_col_fraction = str_col_fraction,\n",
    "                                crs_proj = 3857, \n",
    "                                crs_geographic = 4326)\n",
    "# NOTE: When there are multiple subdivisions we add an integer suffix\n",
    "\n",
    "cities_gdf = add_suffix_to_repeated_values(cities_gdf, str_col_comuni_name)\n",
    "\n",
    "# NOTE: We associate population as the fraction of the area of that sub-region times the total population of the city\n",
    "cities_gdf = redistribute_population_by_fraction(cities_gdf, \n",
    "                                    str_col_popolazione_totale = str_population_col_grid, \n",
    "                                    str_col_fraction = str_col_fraction,\n",
    "                                    str_col_comuni_name = str_col_comuni_name,\n",
    "                                    conserve_total=True)\n",
    "\n",
    "# ------- Users Profiles ------- #                                                                                                                                  # NOTE: here you can define what is the list of user profiles you want to consider\n",
    "#UserProfiles = attendences_foreigners[str_user_profile_vodafone_col].unique().to_list()                                                                             # list of the user profiles ['VISITOR', 'TOURIST', 'COMMUTER', 'INHABITANT']\n",
    "#UserProfiles.append(\"AGGREGATED\")                                                                                                                                   # add the aggregated user profile -> NOTE: the analysis for fluxes will be done on all these.\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "548f81e6",
   "metadata": {},
   "source": [
    "# Road Network  "
   ]
  },
  {
   "cell_type": "code",
   "id": "19bd6600",
   "metadata": {},
   "source": [
    "if False:\n",
    "    import logging\n",
    "    import osmnx as ox\n",
    "    ox.utils.log('INFO')\n",
    "    config.update(extract_bbox_with_buffer(cities_gdf_trentino, buffer_degrees=0.1))\n",
    "    complete_path_transport_gdf = config[f\"{str_prefix_complete_path}_{str_name_gdf_transport}\"]                                                                        # path to the gdf transport\n",
    "    complete_path_transport_graph = config[f\"{str_prefix_complete_path}_{str_name_graph_transport}\"]                                                                    # path to the graph transport\n",
    "    gdf_road_network, G_road_network = pipeline_get_transport_network(config,                                                                                           # get the road network, save it in the output folder                     \n",
    "                                                                    crs,                                                                                                # coordinate reference system                          \n",
    "                                                                    str_transport_idx,                                                                                  # transport index               \n",
    "                                                                    complete_path_transport_gdf,                                                                        # complete path to the transport gdf\n",
    "                                                                    complete_path_transport_graph)                                                                      # compute the transport network, save it in the output folder \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c67bf5d5",
   "metadata": {},
   "source": [
    "# Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "id": "48c431d5",
   "metadata": {},
   "source": [
    "# ---------- Distance and Direction Matrix ------------ #\n",
    "config[str_name_distance_matrix] = os.path.join(config[str_dir_output],f\"{str_name_distance_matrix}.parquet\")      # path to the distance matrix file\n",
    "complete_path_direction_distance_df = config[str_name_distance_matrix]                                                                                          # path to the distance matrix file\n",
    "print(f\"Compute distance and direction matrix: {complete_path_direction_distance_df}\")                                                                          # path to the distance matrix file\n",
    "direction_matrix,distance_matrix = compute_direction_matrix_optimized(cities_gdf,                                                                                #\n",
    "                                    str_centroid_x_col = str_centroid_lat,                                                                                                        #\n",
    "                                    str_centroid_y_col = str_centroid_lon,                                                                                                        #\n",
    "                                    complete_path_direction_distance_df = complete_path_direction_distance_df\n",
    "                                    )                                                                                        # compute the distance and direction matrix, save it in the output folder\n",
    "df_distance_matrix = direction_distance_2_df(direction_matrix,                                                                                                  #                               \n",
    "                                             distance_matrix,                                                                                                   #\n",
    "                                             complete_path_direction_distance_df\n",
    "                                             )   \n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c5050481",
   "metadata": {},
   "source": [
    "# Dictionaries Handling Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "ea989f9c",
   "metadata": {},
   "source": [
    "print(\"Extract list of files from bucket...\")\n",
    "list_files_od, list_files_presenze, list_str_dates_yyyymm = extract_filenames_and_date_from_bucket(bucket)\n",
    "print(\"Extract days available for analysis flows...\")\n",
    "list_all_avaliable_days_flows = extract_all_days_available_analysis_flows_from_raw_dataset(list_files_od,col_str_day_od,str_period_id_presenze,col_str_is_week,s3)                             # NOTE: Needed to create dict_column_flows,grid for post-processing \n",
    "# NOTE: Preparing the dicts that will hold the results of the analysis\n",
    "#dict_column_flows, dict_column_grid, dict_output_hotspot_analysis = initialize_dicts_that_hold_grid_flows_columns_and_hotspot_analysis(list_all_avaliable_days_flows = list_all_avaliable_days_flows, \n",
    "#                                                                                                                                        list_time_intervals = list_time_intervals,\n",
    "#                                                                                                                                        UserProfiles = UserProfiles,\n",
    "#                                                                                                                                        week_days = week_days,\n",
    "#                                                                                                                                        case_2_is_in_flow = case_2_is_in_flow,\n",
    "#                                                                                                                                        case_pipeline = CASE_PIPELINE_AGGREGATION_DAY_HOUR_USER_WEEKDAY)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2bd6c2cb",
   "metadata": {},
   "source": [
    "# Null Day"
   ]
  },
  {
   "cell_type": "code",
   "id": "b6da5845",
   "metadata": {},
   "source": [
    "map_idx_cities_gdf_2_area_code = dict(zip(cities_gdf.index, cities_gdf[str_area_id_presenze]))  # create a map from the index of the cities gdf to the area code\n",
    "# NOTE: Add column indices from AREA_CODE that is the one characteristic of df_presenze, df_od, but not to the case of generated  by gravity flows\n",
    "df_distance_matrix = add_column_area_code_OD_df_distance(df_distance_matrix,\n",
    "                                                        map_idx_cities_gdf_2_area_code,\n",
    "                                                        str_col_origin=str_col_origin,\n",
    "                                                        str_col_destination=str_col_destination,\n",
    "                                                        str_area_code_origin_col=str_area_code_origin_col,\n",
    "                                                        str_area_code_destination_col=str_area_code_destination_col\n",
    "                                                        )  # add the area code to the origin and destination columns of the distance matrix\n",
    "\n",
    "# NOTE: Extract the null day\n",
    "print(\"Initialize null day OD-presenze...\")\n",
    "df_presenze_null_days = extract_presences_vodafone_from_bucket(s3,list_files_presenze, 2)                                                                                                      # NOTE: download the file from the bucket\n",
    "# NOTE: Extract OD\n",
    "df_od_null_days = extract_od_vodafone_from_bucket(s3,list_files_od, 2) \n",
    "df_od_null_days = add_column_is_week_and_str_day(df_od = df_od_null_days,\n",
    "                                str_period_id_presenze = str_period_id_presenze,\n",
    "                                col_str_day_od = col_str_day_od,\n",
    "                                col_str_is_week = col_str_is_week,\n",
    "                                is_null_day = True)\n",
    "\n",
    "\n",
    "df_od_null_days = join_Tij_Vodafone_with_distance_matrix(df_od=df_od_null_days,\n",
    "                                                df_distance_matrix = df_distance_matrix,\n",
    "                                                str_origin_od = str_origin_od,                                   # NOTE: origin area (ITA.<code>)\n",
    "                                                str_destination_od = str_destination_od,                              # NOTE: destination area (ITA.<code>)    \n",
    "                                                str_area_code_origin_col = str_area_code_origin_col,\n",
    "                                                str_area_code_destination_col = str_area_code_destination_col)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Tij_dist_baseline_init = pipeline_initial_df_flows_aggregation_on_dat_hour_user_weekday(df = Tij_dist_baseline_init,\n",
    "                                                                                    tuple_filters = (pl.col(str_trip_type_od) != \"out-out\",\n",
    "                                                                                                        pl.col(str_trip_type_od) != \"in-out\"),\n",
    "                                                                                    message_filters = (f\"{str_trip_type_od} != out-out\",\n",
    "                                                                                                        f\"{str_trip_type_od} != in-out\"),\n",
    "                                                                                    list_columns_groupby = conditioning_2_columns_to_hold_when_aggregating[\"hour_user_weekday\"],            # NOTE: T_{origin}_{destination}_{hour}_{user_profile}_{str_day}_{is_weekday}\n",
    "                                                                                    str_col_trips_to_be_aggregated = \"TRIPS\",\n",
    "                                                                                    str_col_name_aggregated = \"TRIPS\",\n",
    "                                                                                    method_aggregation=\"sum\"\n",
    "                                                                                    )\n",
    "\"\"\"\n",
    "# Memory management: Force garbage collection after loading large datasets\n",
    "gc.collect()   \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6adce520",
   "metadata": {},
   "source": [
    "pl.read_parquet(\"/home/aamaduzzi/aixpa-overtourism-backend/scripts_overtoruism_analysis/Output/Vodafone-Data/grid_all_columns_user.parquet\").columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "34a11c3f",
   "metadata": {},
   "source": [
    "pl.read_parquet(\"/home/aamaduzzi/aixpa-overtourism-backend/scripts_overtoruism_analysis/Output/Vodafone-Data/grid_all_columns_weekday.parquet\").columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bf905b23",
   "metadata": {},
   "source": [
    "# Analysis Average"
   ]
  },
  {
   "cell_type": "code",
   "id": "6e2238a7",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "import polars as pl\n",
    "is_normalize = False\n",
    "# Example filters\n",
    "is_weekday = \"Feriale\"   # could also be [\"Feriale\", \"Prefestivo\"]\n",
    "hour_filter = [7, 8, 25]     # list of allowed hours\n",
    "col_output_total_trips = \"total_trips\"\n",
    "df_concat = concat_df_od_and_add_columns(list_files_od, s3, str_period_id_presenze, col_str_day_od, col_str_is_week)  \n",
    "Tij_dist = df_concat.filter(pl.col(str_trip_type_od) != \"out-out\",\n",
    "                            pl.col(str_trip_type_od) != \"in-out\")\n",
    "# Tij_dist = Tij_dist.filter(pl.col(col_str_is_week) == is_weekday)                                        # 1. filter the dataframe to keep only the rows with the correct weekday/weekend\n",
    "\n",
    "Tij_dist = Tij_dist.group_by([str_origin_od, str_destination_od, str_departure_hour_od, str_origin_visitor_class_id_od, col_str_day_od, col_str_is_week]).agg(\n",
    "        total_trips=pl.col(\"TRIPS\").sum()                                                                                           # 2. aggregate trips across TRIP_TYPE and NATIONALITY_CLASS_ID\n",
    "    ).group_by([str_origin_od, str_destination_od, str_departure_hour_od, str_origin_visitor_class_id_od, col_str_is_week]).agg(\n",
    "        average_trips_over_days=pl.col(col_output_total_trips).mean()                                                               # 3. compute average over days\n",
    "    )\n",
    "\n",
    "for is_weekday in week_days:\n",
    "    fig,ax = plt.subplots(figsize=(10,6))\n",
    "    User2Marker = {user_profile: marker for user_profile, marker in zip(UserProfiles, ['o', 's', '^', 'D', 'x'])}\n",
    "    Type_user_2_count = {user_profile: np.zeros(len(hour_ids)) for user_profile in UserProfiles}\n",
    "    Average_count = np.zeros(len(hour_ids))\n",
    "    n_type_users_considered = 0\n",
    "    for i, user_profile in enumerate(UserProfiles):\n",
    "        if user_profile == \"AGGREGATED\":\n",
    "            continue\n",
    "        else:\n",
    "            for j, hour_id in enumerate(hour_ids):\n",
    "                Type_user_2_count[user_profile][j] = Tij_dist.filter(pl.col(str_departure_hour_od) == hour_id,                                      # \n",
    "                                                                    pl.col(str_origin_visitor_class_id_od) == UserProfile2IndexVodafone[user_profile],\n",
    "                                                                    pl.col(\"is_weekday\") == is_weekday).select(pl.col(\"average_trips_over_days\")).to_numpy().sum()\n",
    "                Average_count[j] += Type_user_2_count[user_profile][j]\n",
    "            if is_normalize:\n",
    "                Type_user_2_count[user_profile] /= Type_user_2_count[user_profile].sum()\n",
    "            ax.scatter(hour_ids, Type_user_2_count[user_profile], label=user_profile, marker=User2Marker[user_profile], s=50)\n",
    "            ax.set_title(f\"total trips per user profile {is_weekday}\")\n",
    "            ax.set_xlabel(\"Hour of Day\")\n",
    "            ax.set_ylabel(\"Fraction total Trips\")\n",
    "            n_type_users_considered += 1\n",
    "    if n_type_users_considered > 0:\n",
    "        Average_count /= n_type_users_considered\n",
    "        if is_normalize:\n",
    "            Average_count /= Average_count.sum()\n",
    "        ax.scatter(hour_ids, Average_count, label=\"Average\", marker='*', s=200, color='black')\n",
    "    ax.grid(True)\n",
    "    plt.legend()\n",
    "    dir_output_average = os.path.join(config[\"dir_output\"],is_weekday)\n",
    "    os.makedirs(dir_output_average, exist_ok=True)\n",
    "    plt.savefig(os.path.join(dir_output_average,f\"total_trips_per_user_profile_{is_weekday}.png\"))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3f204a98",
   "metadata": {},
   "source": [
    "# Pipeline Average over all days and Hours"
   ]
  },
  {
   "cell_type": "code",
   "id": "0c40c5ce",
   "metadata": {},
   "source": [
    "\n",
    "        # ------------------------------------------------------\n",
    "        # ⬇️ Put your full analysis pipeline here (the big block)\n",
    "        # Replace your hardcoded values with these loop vars:\n",
    "        # - str_day\n",
    "        # - time_interval\n",
    "        # - user_profile\n",
    "        # - is_weekday\n",
    "        # ------------------------------------------------------\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "54020fff",
   "metadata": {},
   "source": [
    "# NOTE: Initialize the dataframe flows from the concatenation of all the OD files\n",
    "\n",
    "\n",
    "df_concat = concat_df_od_and_add_columns(list_files_od, \n",
    "                                         s3, \n",
    "                                         str_period_id_presenze, \n",
    "                                         col_str_day_od, \n",
    "                                         col_str_is_week)  \n",
    "list_unique_days_od = df_concat[col_str_day_od].unique().to_list()\n",
    "\n",
    "# NOTE: Prepare the dataframe to give to the prepare_flow_dataframe_for_hierarchical_prcedure: essentially here it considers just the in-in and out-in trips, and joins with the distance matrix -> i,j col are given\n",
    "df_od_with_just_in_in_out_in_trips = default_initial_preparation_common_to_all_cases_df_flows_not_baseline(df_od = df_concat,\n",
    "                                                                          df_distance_matrix = df_distance_matrix,\n",
    "                                                                          str_origin_od = str_origin_od,\n",
    "                                                                          str_destination_od = str_destination_od,\n",
    "                                                                          str_area_code_origin_col = str_area_code_origin_col,\n",
    "                                                                          str_area_code_destination_col = str_area_code_destination_col,\n",
    "                                                                          str_trip_type_od = str_trip_type_od)\n",
    "\n",
    "\n",
    "columns_2_hold_geopandas_base = [str_area_id_presenze,str_centroid_lat,str_centroid_lon,str_col_comuni_name,str_grid_idx,\"geometry\"]                        # base columns to hold in the geopandas\n",
    "columns_hold_output_hotspot_df_base = [str_area_id_presenze]\n",
    "\n",
    "# Save the output at the base level of\n",
    "cities_gdf[str_grid_idx] = cities_gdf.index  # add the grid idx as a column\n",
    "cities_gdf[columns_2_hold_geopandas_base].to_file(os.path.join(config[str_dir_output],f\"cities_gdf_base_columns_{case_pipeline}.geojson\"),driver=\"GeoJSON\")  # save the base columns of the cities gdf\n",
    "#for case_pipeline in conditioning_2_columns_to_hold_when_aggregating.keys():\n",
    "#    print(f\"Case pipeline: {case_pipeline}\")\n",
    "\n",
    "global_Tij_holding_all_columns_flows = None \n",
    "global_cities_gdf_holding_all_columns_flows = cities_gdf\n",
    "\n",
    "\n",
    "case_pipeline = \"weekday\"\n",
    "\n",
    "\n",
    "dict_column_flows_aggregation_weekday, dict_column_grid_aggregation_weekday, dict_output_hotspot_analysis_aggregation_weekday = initialize_dicts_that_hold_grid_flows_columns_and_hotspot_analysis(list_all_avaliable_days_flows = list_all_avaliable_days_flows, \n",
    "                                                                                                                                                                                                    list_time_intervals = list_time_intervals,\n",
    "                                                                                                                                                                                                    UserProfiles = UserProfiles,\n",
    "                                                                                                                                                                                                    week_days = week_days,\n",
    "                                                                                                                                                                                                    case_2_is_in_flow = case_2_is_in_flow,\n",
    "                                                                                                                                                                                                    case_pipeline = case_pipeline)\n",
    "# NOTE: Aggregate the dataframe at the level you need\n",
    "Tij_dist_init, Tij_dist_baseline_init = prepare_flow_dataframe_for_hierarchical_prcedure(df_od_with_just_in_in_out_in_trips = df_od_with_just_in_in_out_in_trips,\n",
    "                                                                                        Tij_dist_baseline_init = df_od_null_days ,\n",
    "                                                                                        case = case_pipeline ,\n",
    "                                                                                        user_profile = \"\")\n",
    "\n",
    "\n",
    "\n",
    "for is_weekday in week_days:\n",
    "    # NOTE: Directory to save the output\n",
    "    str_dir_output_date = os.path.join(os.path.join(config[str_dir_output], is_weekday))  # NOTE: create a directory for the date\n",
    "    Path(str_dir_output_date).mkdir(parents=True, exist_ok=True)                                                                                                # create the directory if it does not exist                \n",
    "    idx_case = 0\n",
    "    columns_2_hold_geopandas_base = [str_area_id_presenze,str_centroid_lat,str_centroid_lon,str_col_comuni_name,str_grid_idx,\"geometry\"]                        # base columns to hold in the geopandas\n",
    "    str_day = \"\"                                                                                                                                            # NOTE: here we do not consider a specific day\n",
    "    int_hour_start_window_interest = int_hour_start_window_interest                                                                                     # NOTE: here we do not consider a specific hour\n",
    "    user_profile = \"average_all_days\"                                                                                                                                        # NOTE: here we do not consider a specific user profile\n",
    "    time_interval = [0]                                                                                                                                        # NOTE: here we do not consider a specific time\n",
    "    str_t = \"\"\n",
    "    str_t1 = \"\"\n",
    "    for suffix_in,is_in_flows in case_2_is_in_flow.items():                                                   \n",
    "        # NOTE: Pick the selected columns for the analysis\n",
    "        Tij_dist,Tij_dist_baseline = filter_flows_by_conditions_from_cases(Tij_dist_init = Tij_dist_init,\n",
    "                                                                Tij_dist_baseline_init = Tij_dist_baseline_init,\n",
    "                                                                case_analysis = case_pipeline,\n",
    "                                                                is_weekday = is_weekday,\n",
    "                                                                day_id_of_interest = str_day,\n",
    "                                                                hour_id_of_interest = int_hour_start_window_interest,\n",
    "                                                                user_profile = user_profile\n",
    "                                                                )\n",
    "        dict_column_flows_aggregation_weekday, dict_column_grid_aggregation_weekday, extension_columns_2_hold, columns_2_hold_geopandas_for_flows_plot, columns_flows_2_be_merged_2_keep, on_colums_flows_2_join, on_columns_grid_2_join = define_columns_to_hold_and_merge_both_for_grid_and_flows_OD_analysis(columns_2_hold_geopandas_base = columns_2_hold_geopandas_base,\n",
    "                                                                                                                                                                                                                                                                        str_col_origin = str_col_origin,\n",
    "                                                                                                                                                                                                                                                                        str_col_destination = str_col_destination,\n",
    "                                                                                                                                                                                                                                                                        str_grid_idx = str_grid_idx,\n",
    "                                                                                                                                                                                                                                                                        dict_column_flows = dict_column_flows_aggregation_weekday,\n",
    "                                                                                                                                                                                                                                                                        dict_column_grid = dict_column_grid_aggregation_weekday,\n",
    "                                                                                                                                                                                                                                                                        str_day = str_day,\n",
    "                                                                                                                                                                                                                                                                        time_interval = time_interval,\n",
    "                                                                                                                                                                                                                                                                        user_profile = user_profile,\n",
    "                                                                                                                                                                                                                                                                        is_weekday = is_weekday,\n",
    "                                                                                                                                                                                                                                                                        suffix_in = suffix_in,\n",
    "                                                                                                                                                                                                                                                                        str_t = str_t,\n",
    "                                                                                                                                                                                                                                                                        str_t1 = str_t1,\n",
    "                                                                                                                                                                                                                                                                        case_pipeline = case_pipeline)\n",
    "        \n",
    "\n",
    "        # NOTE: Extract columns that are for the analysis\n",
    "        str_col_n_trips, str_col_n_trips_baseline, str_col_difference_baseline, str_col_total_flows_grid = extract_name_columns_for_difference_pipeline(dict_column_flows = dict_column_flows_aggregation_weekday,\n",
    "                                                                                                                                                        dict_column_grid = dict_column_grid_aggregation_weekday,str_day = str_day,time_interval = time_interval,\n",
    "                                                                                                                                                        user_profile = user_profile,is_weekday = is_weekday,\n",
    "                                                                                                                                                        is_in_flows = is_in_flows,suffix_in = suffix_in,case_pipeline = case_pipeline)\n",
    "        Tij_dist = Tij_dist.with_columns((pl.col(\"TRIPS\")).alias(str_col_n_trips))  # Divide by two the trips since we are considering both nationalities\n",
    "        Tij_dist_baseline = Tij_dist_baseline.with_columns(pl.col(\"TRIPS\").alias(str_col_n_trips_baseline))                # NOTE: rename the column with the trips to the one needed for the analysis\n",
    "        print(f\"Compare Tij_dist to Tij_dist_baseline: {suffix_in}\")\n",
    "        Tij_dist = compute_difference_trips_col_day_baseline(Tij_dist = Tij_dist,\n",
    "                                                            Tij_dist_baseline = Tij_dist_baseline,\n",
    "                                                            str_col_n_trips = str_col_n_trips,\n",
    "                                                            str_col_n_trips_baseline = str_col_n_trips_baseline,\n",
    "                                                            str_col_difference_baseline = str_col_difference_baseline,\n",
    "                                                            str_col_origin = str_col_origin,\n",
    "                                                            str_col_destination = str_col_destination,\n",
    "                                                            on_colums_flows_2_join = on_colums_flows_2_join\n",
    "                                                            )\n",
    "\n",
    "        # NOTE: Step 1 Start the Hierarchical Analysis\n",
    "        if idx_case == 0:\n",
    "            geojson_input_hierarchy = cities_gdf\n",
    "        else:\n",
    "            geojson_input_hierarchy = mh.grid\n",
    "        # NOTE: Mobility Hierarchy Analysis - Inflows and Outflows -> Reformat Input and choose outside what it is going to be (Either in or out)\n",
    "        mh, hotspot_2_origin_idx_2_crit_dest_idx, hotspot_flows, list_indices_all_fluxes_for_colormap = pipeline_mobility_hierarchy_time_day_type_trips(cities_gdf = geojson_input_hierarchy,Tij_dist_fit_gravity = Tij_dist,str_population_col_grid = str_population_col_grid,\n",
    "                                                                                                                                                        str_col_comuni_name = str_col_comuni_name,str_col_origin = str_col_origin,str_col_destination = str_col_destination,\n",
    "                                                                                                                                                        str_col_n_trips = str_col_n_trips,\n",
    "                                                                                                                                                        str_col_total_flows_grid = str_col_total_flows_grid,\n",
    "                                                                                                                                                        str_hotspot_prefix = str_hotspot_prefix,str_centroid_lat = str_centroid_lat,str_centroid_lon = str_centroid_lon,\n",
    "                                                                                                                                                        str_grid_idx = str_grid_idx,user_profile = user_profile,str_t = str_t,str_t1 = str_t1,is_in_flows = is_in_flows,                                                                                                  # NOTE: is_in_flows = True means that we are considering the incoming fluxes to the hotspot\n",
    "                                                                                                                                                        columns_2_hold_geopandas = columns_2_hold_geopandas_for_flows_plot,int_levels = 7)\n",
    "        \n",
    "        dict_output_hotspot_analysis_aggregation_weekday = fill_dict_output_hotspot_analysis_OD_analysis_from_case_pipeline(\n",
    "                                                                                                                            dict_output_hotspot_analysis = dict_output_hotspot_analysis_aggregation_weekday,\n",
    "                                                                                                                            str_day = str_day,\n",
    "                                                                                                                            time_interval = time_interval,\n",
    "                                                                                                                            user_profile = user_profile,\n",
    "                                                                                                                            is_weekday = is_weekday,\n",
    "                                                                                                                            is_in_flows = is_in_flows,\n",
    "                                                                                                                            suffix_in = suffix_in,\n",
    "                                                                                                                            case_pipeline = case_pipeline,\n",
    "                                                                                                                            hotspot_2_origin_idx_2_crit_dest_idx = hotspot_2_origin_idx_2_crit_dest_idx,\n",
    "                                                                                                                            list_indices_all_fluxes_for_colormap = list_indices_all_fluxes_for_colormap,\n",
    "                                                                                                                            hotspot_flows = hotspot_flows\n",
    "                                                                                                                            )   \n",
    "        hotspot_levels = get_values_from_case_pipeline_OD_analysis(dict_column_flows = None,dict_column_grid = None,dict_output_hotspot_analysis = dict_output_hotspot_analysis_aggregation_weekday,str_day = str_day,time_interval = time_interval,user_profile = user_profile,is_weekday = is_weekday,is_in_flows = is_in_flows,suffix_in = suffix_in,name_dict = \"dict_output_hotspot_analysis\",name_key = \"hotspot_levels\",case_pipeline = case_pipeline)\n",
    "        print(f\"Map flux generated for {suffix_in} flows\")\n",
    "        # NOTE: Save the maps for incoming fluxes -> NOTE: The str_output_dir_date is unique \n",
    "        save_output_mobility_hierarchy_dependent_is_in_fluxes(\n",
    "                                                            str_dir_output_date = str_dir_output_date,\n",
    "                                                            map_hierarchy = mh.fmap,\n",
    "                                                            user_profile = user_profile,\n",
    "                                                            hotspot_levels = hotspot_levels,\n",
    "                                                            hotspot_2_origin_idx_2_crit_dest_idx = hotspot_2_origin_idx_2_crit_dest_idx,\n",
    "                                                            str_t = str_t,\n",
    "                                                            str_t1 = str_t1,\n",
    "                                                            is_in_flows = is_in_flows                                                                                                  # NOTE: is_in_flows = True means that we are considering the incoming fluxes to the hotspot\n",
    "                                                                )                                                                                                             # show the map in the browser\n",
    "        # Memory management: Clear map_flux after saving\n",
    "        gc.collect()                                        \n",
    "        # NOTE: Global save the flows and cities_gdf by merging the str_day, str_t, str_t1, user_profile\n",
    "        print(f\"Case: {suffix_in} - mh.flows: {mh.flows.columns}\\nflows merged: {Tij_dist.columns}\\nmh.grid: {mh.grid.columns}\")\n",
    "        \n",
    "        # Ensure AREA_ID has the same type before merging\n",
    "        if str_area_id_presenze in global_cities_gdf_holding_all_columns_flows.columns:\n",
    "            global_cities_gdf_holding_all_columns_flows[str_area_id_presenze] = global_cities_gdf_holding_all_columns_flows[str_area_id_presenze].astype(str)\n",
    "        if str_area_id_presenze in mh.grid.columns:\n",
    "            mh.grid[str_area_id_presenze] = mh.grid[str_area_id_presenze].astype(str)\n",
    "\n",
    "        global_Tij_holding_all_columns_flows, global_cities_gdf_holding_all_columns_flows = merge_flows_and_grid_with_global_to_obtain_unique_dfs(global_Tij_holding_all_columns_flows = global_Tij_holding_all_columns_flows,\n",
    "                                                                                                                                                    flows_2_be_merged = Tij_dist,\n",
    "                                                                                                                                                    global_cities_gdf_holding_all_columns_flows = global_cities_gdf_holding_all_columns_flows,\n",
    "                                                                                                                                                    grid_single_case_2_be_merged = mh.grid,\n",
    "                                                                                                                                                    columns_join_global_geopandas = columns_2_hold_geopandas_for_flows_plot,\n",
    "                                                                                                                                                    columns_flows_2_be_merged_2_keep = columns_flows_2_be_merged_2_keep,\n",
    "                                                                                                                                                    on_columns_flows_2_join = on_colums_flows_2_join,\n",
    "                                                                                                                                                    on_columns_grid_2_join = on_columns_grid_2_join,\n",
    "                                                                                                                                                    message_geojson = f\"average {suffix_in} grid\",\n",
    "                                                                                                                                                    message_flows = f\"average {suffix_in} flows\",\n",
    "                                                                                                                                                    )     \n",
    "        grid_global = pl.from_pandas(global_cities_gdf_holding_all_columns_flows.copy().drop(\"geometry\", axis=1))                               \n",
    "        # Memory management: Clear baseline analysis variables\n",
    "        gc.collect()\n",
    "        idx_case += 1\n",
    "        print(f\"Number columns global flows after join: \",len(global_Tij_holding_all_columns_flows.columns),f\" cities: \",len(global_cities_gdf_holding_all_columns_flows.columns))\n",
    "    # NOTE: Visualize All Outputs\n",
    "    for suffix_in,is_in_flows in case_2_is_in_flow.items():                                                   \n",
    "        hotspot_2_origin_idx_2_crit_dest_idx, str_col_total_flows_grid, str_col_hotspot_level, str_col_n_trips, str_caption_colormap, str_col_difference = extract_name_columns_for_hierarchical_plot(dict_column_flows = dict_column_flows_aggregation_weekday,dict_column_grid = dict_column_grid_aggregation_weekday,\n",
    "                                                                                                                                                                                                      dict_output_hotspot_analysis = dict_output_hotspot_analysis_aggregation_weekday,\n",
    "                                                                                                                                                                                                    str_day = str_day,time_interval = time_interval,user_profile = user_profile,is_weekday = is_weekday,is_in_flows = is_in_flows,\n",
    "                                                                                                                                                                                                    suffix_in = suffix_in,case_pipeline = case_pipeline)\n",
    "        \n",
    "        map_flux = visualize_critical_fluxes_with_lines(grid = global_cities_gdf_holding_all_columns_flows,\n",
    "                                                        df_flows = global_Tij_holding_all_columns_flows,\n",
    "                                                        hotspot_2_origin_idx_2_crit_dest_idx = hotspot_2_origin_idx_2_crit_dest_idx,                        # NOTE:\n",
    "                                                        str_col_total_flows_grid = str_col_total_flows_grid,\n",
    "                                                        str_col_hotspot = str_col_hotspot_level,\n",
    "                                                        str_col_n_trips = str_col_n_trips,\n",
    "                                                        is_in_flows = is_in_flows,\n",
    "                                                        str_col_origin = str_col_origin,\n",
    "                                                        str_col_destination = str_col_destination,\n",
    "                                                        str_centroid_lat = str_centroid_lat,\n",
    "                                                        str_centroid_lon = str_centroid_lon,\n",
    "                                                        str_col_comuni_name= str_col_comuni_name,\n",
    "                                                        str_grid_idx = str_grid_idx,\n",
    "                                                        str_caption_colormap = str_caption_colormap,\n",
    "                                                        str_colormap= 'YlOrRd'\n",
    "                        )\n",
    "        complete_path_map = os.path.join(str_dir_output_date,f\"map_fluxes_{user_profile}_{suffix_in}_{str_day}_{str_t}_{str_t1}_{str_day}.html\")                                                # path to the map file\n",
    "        with open(os.path.join(str_dir_output_date,f\"dict_column_flows_aggregation_weekday_{user_profile}_{suffix_in}_t_{str_t}_{str_t1}_d_{str_day}.json\"), 'w') as f:\n",
    "            json.dump(dict_column_flows_aggregation_weekday, f, indent=4)\n",
    "        with open(os.path.join(str_dir_output_date,f\"dict_column_grid_aggregation_weekday_{user_profile}_{suffix_in}_t_{str_t}_{str_t1}_d_{str_day}.json\"), 'w') as f:\n",
    "            json.dump(dict_column_grid_aggregation_weekday, f, indent=4)\n",
    "        with open(os.path.join(str_dir_output_date,f\"dict_output_hotspot_analysis_aggregation_weekday_{user_profile}_{suffix_in}_t_{str_t}_{str_t1}_d_{str_day}.json\"), 'w') as f:\n",
    "            json.dump(dict_output_hotspot_analysis_aggregation_weekday, f, indent=4)\n",
    "        try:\n",
    "            map_flux.save(complete_path_map)                                                                                                                         # save the map to the output folder\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving map_flux: {e}\")\n",
    "global_Tij_holding_all_columns_flows.write_parquet(os.path.join(str_dir_output_date,f\"Tij_all_columns_{case_pipeline}.parquet\"))                                      # save the global flows\n",
    "grid_global.write_parquet(os.path.join(str_dir_output_date,f\"grid_all_columns_{case_pipeline}.parquet\"))  # save the global grid\n",
    "#    global_cities_gdf_holding_all_columns_flows.to_file(os.path.join(str_dir_output_date,f\"global_cities_gdf_holding_all_columns_flows.geojson\"),driver='GeoJSON')  # save the global grid"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c89e560a",
   "metadata": {},
   "source": [
    "# Analysis Fluxes: Hotspots Diffusion 1-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d2ea16",
   "metadata": {},
   "source": [
    "##  Global Variables Pipeline - Setting the stage for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "id": "9c722420",
   "metadata": {},
   "source": [
    "\n",
    "for i,file in tqdm(enumerate(list_files_od),desc=\"Files OD Vodafone\"):                                                                                           # for each file in the list of files\n",
    "    print(\"Processing file: \", file,f\" iter: {i}\")                                                                                                                                          # print the file name\n",
    "    # NOTE: Define the null day being october 2024 -> Avoid the analysis on it since we are considering just the summer days in relation to it.\n",
    "    if file == 'projects/tourism/_origin/vodafone-aixpa/od-mask_202410.parquet':\n",
    "        is_null_day = True\n",
    "    else:\n",
    "        is_null_day = False\n",
    "    if is_null_day:\n",
    "        continue\n",
    "    else:\n",
    "        # NOTE: Extract the presenze and OD data from the bucket -> otherwise generate the flows from gravitational model\n",
    "        if not is_generate_fluxes:\n",
    "            # NOTE: Extract presenze\n",
    "            df_presenze = extract_presences_vodafone_from_bucket(s3,list_files_presenze, i)                                                                                                      # NOTE: download the file from the bucket\n",
    "            # NOTE: Extract OD\n",
    "            df_od = extract_od_vodafone_from_bucket(s3,list_files_od, i) \n",
    "            # Add the column\n",
    "            if \"202410\" in file:\n",
    "                is_null_day = True\n",
    "            else:\n",
    "                is_null_day = False\n",
    "\n",
    "            # NOTE: Fill info geodataframe with presences \n",
    "#            df_presenze_pd = df_presenze.to_pandas()  # Convert to pandas first: add -> AREA_CODE\n",
    "#            df_presenze_unique = df_presenze_pd.drop_duplicates(subset=[str_area_id_presenze], keep='first')\n",
    "#            cities_gdf = cities_gdf.merge(df_presenze_unique[str_area_id_presenze], left_on=str_area_id_presenze,right_on=str_area_id_presenze,how=\"left\")\n",
    "            # NOTE: Add the column is_week and str_day to the df_od\n",
    "            df_od = add_column_is_week_and_str_day(df_od = df_od,\n",
    "                                                    str_period_id_presenze = str_period_id_presenze,\n",
    "                                                    col_str_day_od = col_str_day_od,\n",
    "                                                    col_str_is_week = col_str_is_week,\n",
    "                                                    is_null_day = is_null_day)  \n",
    "            list_unique_days_od = df_od[col_str_day_od].unique().to_list()\n",
    "            print(f\"Extracted dates analysis: {list_unique_days_od}\")            \n",
    "            # NOTE: Join the distance matrix with the flows -> add the columns that hold information about the unit vector and the distance between O-D\n",
    "            Tij_dist_init = join_Tij_Vodafone_with_distance_matrix(df_od=df_od,\n",
    "                                                            df_distance_matrix = df_distance_matrix,\n",
    "                                                            str_origin_od = str_origin_od,                                   # NOTE: origin area (ITA.<code>)\n",
    "                                                            str_destination_od = str_destination_od,                              # NOTE: destination area (ITA.<code>)    \n",
    "                                                            str_area_code_origin_col = str_area_code_origin_col,\n",
    "                                                            str_area_code_destination_col = str_area_code_destination_col)\n",
    "                        \n",
    "            # NOTE: Initialize the dataframe flows to the form where the trips are the sum over all observations ut still are conditioned to the day, hour, user profile and weekday/weekend\n",
    "            Tij_dist_init = pipeline_initial_df_flows_aggregation_on_dat_hour_user_weekday(df = Tij_dist_init,\n",
    "                                                                                      tuple_filters = (pl.col(str_trip_type_od) != \"out-out\",\n",
    "                                                                                                        pl.col(str_trip_type_od) != \"in-out\"),\n",
    "                                                                                      message_filters = (f\"{str_trip_type_od} != out-out\",\n",
    "                                                                                                         f\"{str_trip_type_od} != in-out\"),\n",
    "                                                                                      list_columns_groupby = conditioning_2_columns_to_hold_when_aggregating[\"day_hour_user_weekday\"],            # NOTE: T_{origin}_{destination}_{hour}_{user_profile}_{str_day}_{is_weekday}\n",
    "                                                                                      str_col_trips_to_be_aggregated = \"TRIPS\",\n",
    "                                                                                      str_col_name_aggregated = \"TRIPS\",\n",
    "                                                                                      method_aggregation=\"sum\"\n",
    "                                                                                     )\n",
    "\n",
    "            # Memory management: Delete intermediate dataframes after processing\n",
    "#            del df_presenze_pd, df_presenze_unique\n",
    "            gc.collect()\n",
    "        else:\n",
    "            list_unique_days_od = [\"2024-09-09\"]                                                                                                                                              # NOTE: the days of interest -> string format\n",
    "            pass\n",
    "        \n",
    "        if not list_unique_days_od:\n",
    "            print(f\"Warning: No days found for file {file}. Skipping...\")\n",
    "            continue\n",
    "        for str_day in list_unique_days_od:\n",
    "             else:        \n",
    "                datetime_day = pd.to_datetime(str_day)     \n",
    "                # NOTE: Time intervals of interest\n",
    "                # NOTE: Time intervals\n",
    "                for time_interval in list_time_intervals:\n",
    "                    print(\"Time interval: \", time_interval)                                                                                                                               # for each time interval\n",
    "                    int_hour_start_window_interest = time_interval[0]                                                                                                                   # start time window of interest\n",
    "                    int_hour_end_window_interest = time_interval[1]                                                                                                                     # end time window of interest\n",
    "                    str_t = str(int_hour_start_window_interest)\n",
    "                    str_t1 = str(int_hour_end_window_interest)\n",
    "                    if int_hour_start_window_interest != 25:                                                                                                                   # for each user profile                                   \n",
    "                        # NOTE: chnge the time according to the source                                                                                                                  \n",
    "                        is_fluxes_hourly = True                                                                                                                         # NOTE: if the fluxes are generated hourly\n",
    "                    else:\n",
    "                        is_fluxes_hourly = False                                                                                                                         # NOTE: if the fluxes are generated hourly\n",
    "                    for is_weekday in week_days:                                                                                                                                            # for each day type (weekday/weekend)\n",
    "                        print(f\"Processing {is_weekday}\")                                                                                                                                              # print the day type\n",
    "                        # NOTE: Days of interest\n",
    "                        # NOTE: Profile user\n",
    "                        print(\"Initialize flows and cities that will hold all columns analysis...\")\n",
    "                        global_cities_gdf_holding_all_columns_flows = cities_gdf.copy()                                                                                                                                # Create a global copy of cities_gdf to hold all columns: name_file_system: flows_all_analysis.parquet\n",
    "                        global_Tij_holding_all_columns_flows = None                                                                                                                                                    # Create a global variable to hold Tij_dist with all columns:                         \n",
    "                        for user_profile in UserProfiles:\n",
    "                            Tij_dist = Tij_dist_init\n",
    "                            Tij_dist_baseline = Tij_dist_baseline_init                                \n",
    "                            print(\"Processing user profile: \", user_profile)                                                                                                                   # for each user profile\n",
    "                            # Initialize variables that are relevant for the time analysis (time and user profile)\n",
    "                            name_week_day = is_weekday                                                                                                                                  # name of the day type\n",
    "                            # NOTE: These are the columns that we hold in the plot of the hierarchy (These are the ones that are in \n",
    "                            # the geojson produced by the MobilityHierarchy that are going to be important for the output plots)\n",
    "                            columns_2_hold_geopandas_base = [str_area_id_presenze,str_centroid_lat,str_centroid_lon,str_col_comuni_name,str_grid_idx,\"geometry\"]                        # base columns to hold in the geopandas\n",
    "                            # NOTE: Directory to save the output\n",
    "                            str_dir_output_date = os.path.join(config[str_dir_output],str_day,f\"{str_t}_{str_t1}\",name_week_day)                                                                      # NOTE: create a directory for the date\n",
    "                            Path(str_dir_output_date).mkdir(parents=True, exist_ok=True)                                                                                                # create the directory if it does not exist                \n",
    "                            idx_case = 0\n",
    "                            if WORK_IN_PROGRESS and os.path.exists(os.path.join(str_dir_output_date, f\"most_critical_directions_{user_profile}_{str_t}_{str_t1}_{str_day}.html\")):\n",
    "                                print(f\"Skipping analysis for {str_day} {str_t}-{str_t1} {user_profile} as output already exists.\")\n",
    "                                continue\n",
    "                            else:\n",
    "                                for suffix_in,is_in_flows in case_2_is_in_flow.items():\n",
    "                                    case_pipeline = \"day_hour_user_weekday\" \n",
    "                                    dict_column_flows, dict_column_grid, extension_columns_2_hold, columns_2_hold_geopandas_for_flows_plot, columns_flows_2_be_merged_2_keep, on_colums_flows_2_join, on_columns_grid_2_join = define_columns_to_hold_and_merge_both_for_grid_and_flows_OD_analysis(columns_2_hold_geopandas_base = columns_2_hold_geopandas_base,\n",
    "                                                                                                                                                                                                                                                                                                    str_col_origin = str_col_origin,\n",
    "                                                                                                                                                                                                                                                                                                    str_col_destination = str_col_destination,\n",
    "                                                                                                                                                                                                                                                                                                    str_grid_idx = str_grid_idx,\n",
    "                                                                                                                                                                                                                                                                                                    dict_column_flows = dict_column_flows,\n",
    "                                                                                                                                                                                                                                                                                                    dict_column_grid = dict_column_grid,\n",
    "                                                                                                                                                                                                                                                                                                    str_day = str_day,\n",
    "                                                                                                                                                                                                                                                                                                    time_interval = time_interval,\n",
    "                                                                                                                                                                                                                                                                                                    user_profile = user_profile,\n",
    "                                                                                                                                                                                                                                                                                                    is_weekday = is_weekday,\n",
    "                                                                                                                                                                                                                                                                                                    suffix_in = suffix_in,\n",
    "                                                                                                                                                                                                                                                                                                    str_t = str_t,\n",
    "                                                                                                                                                                                                                                                                                                    str_t1 = str_t1,\n",
    "                                                                                                                                                                                                                                                                                                    case_pipeline = case_pipeline)\n",
    "                                    # NOTE: Columns that are held by the final geopandas -> The one used for all plots\n",
    "                                    # Generation Fluxes                                                                                                                                           #\n",
    "                                    if is_generate_fluxes:                                                                                                                         # if the fluxes are generated\n",
    "                                        Tij_dist, Tij_dist_baseline = routine_generation_flows(df_distance_matrix,\n",
    "                                                                                                cities_gdf,\n",
    "                                                                                                str_col_i = str_col_origin,\n",
    "                                                                                                str_col_j = str_col_destination,\n",
    "                                                                                                str_population_col = str_population_col_grid,\n",
    "                                                                                                str_population_i_col = str_population_i,\n",
    "                                                                                                str_population_j_col = str_population_j)\n",
    "                                        gc.collect()\n",
    "                                    else:\n",
    "                                        # NOTE: Process vodafone data to match the format of generated fluxes\n",
    "                                        # NOTE: The analysis is unique for (df_od,df_presenze,str_col_n_trips,str_day) -> other variables are needed for\n",
    "                                        # functions and filtering but these steps are needed for the comparison of the baseline and the day of analysis.\n",
    "                                        Tij_dist,Tij_dist_baseline = filter_flows_by_conditions_from_cases(Tij_dist_init = Tij_dist_init,\n",
    "                                                                                                Tij_dist_baseline_init = Tij_dist_baseline_init,\n",
    "                                                                                                case_analysis = case_pipeline,\n",
    "                                                                                                is_weekday = is_weekday,\n",
    "                                                                                                day_id_of_interest = str_day,\n",
    "                                                                                                hour_id_of_interest = int_hour_start_window_interest,\n",
    "                                                                                                user_profile = user_profile\n",
    "                                                                                                )\n",
    "                                        if user_profile != \"AGGREGATED\":\n",
    "                                            pass\n",
    "                                        else:\n",
    "                                            # NOTE: We are summing over all the days as they are summed implicitly in Tij_dist_init (Vodafone gave us the sum over 15 days, here we sum over the user profiles)\n",
    "                                            Tij_dist_baseline = Tij_dist_baseline.with_columns((pl.col(\"TRIPS\")/2).floor().alias(\"TRIPS\"))  # Divide by two the trips since we are considering both nationalities\n",
    "                                        # Memory management: Clear intermediate dataframes\n",
    "                                        str_col_n_trips, str_col_n_trips_baseline, str_col_difference_baseline, str_col_total_flows_grid = extract_name_columns_for_difference_pipeline(dict_column_flows = dict_column_flows,\n",
    "                                                                                                                                                                                        dict_column_grid = dict_column_grid,str_day = str_day,time_interval = time_interval,\n",
    "                                                                                                                                                                                        user_profile = user_profile,is_weekday = is_weekday, \n",
    "                                                                                                                                                                                        is_in_flows = is_in_flows,suffix_in = suffix_in,case_pipeline = case_pipeline)\n",
    "                                        # Change the name of the columns of the flows\n",
    "                                        Tij_dist = Tij_dist.with_columns(pl.col(\"TRIPS\").alias(str_col_n_trips))                                  # NOTE: rename the column with the trips to the one needed for the analysis\n",
    "                                        Tij_dist_baseline = Tij_dist_baseline.with_columns(pl.col(\"TRIPS\").alias(str_col_n_trips_baseline))                # NOTE: rename the column with the trips to the one needed for the analysis\n",
    "                                        print(f\"Compare Tij_dist to Tij_dist_baseline: {str_t}-{str_t1}, {user_profile}, {suffix_in}\")\n",
    "                                        Tij_dist = compute_difference_trips_col_day_baseline(Tij_dist = Tij_dist,\n",
    "                                                            Tij_dist_baseline = Tij_dist_baseline,\n",
    "                                                            str_col_n_trips = str_col_n_trips,\n",
    "                                                            str_col_n_trips_baseline = str_col_n_trips_baseline,\n",
    "                                                            str_col_difference_baseline = str_col_difference_baseline,\n",
    "                                                            str_col_origin = str_col_origin,\n",
    "                                                            str_col_destination = str_col_destination,\n",
    "                                                            on_colums_flows_2_join = on_colums_flows_2_join\n",
    "                                                            )\n",
    "                                        print(f\"Resulting flows to analyze:  {Tij_dist.shape} & baseline: {Tij_dist_baseline.shape} \")\n",
    "                                        print(f\"Total number trips recorded: {Tij_dist[str_col_n_trips].sum()}, & baseline: {Tij_dist_baseline[str_col_n_trips_baseline].sum()}\")                                                                                                            # print the shape of the resulting dataframe\n",
    "                                        print(f\"Total number missing values: {Tij_dist.select(pl.col(str_col_n_trips).is_null().sum()).item()}, & baseline: {Tij_dist_baseline.select(pl.col(str_col_n_trips_baseline).is_null().sum()).item()}\")                                                                                   # print the shape of the resulting dataframe\n",
    "                                        \n",
    "                                        \n",
    "                                        # NOTE: Step 1 Start the Hierarchical Analysis\n",
    "                                        if idx_case == 0:\n",
    "                                            geojson_input_hierarchy = cities_gdf\n",
    "                                        else:\n",
    "                                            geojson_input_hierarchy = mh.grid\n",
    "                                        # NOTE: Mobility Hierarchy Analysis - Inflows and Outflows -> Reformat Input and choose outside what it is going to be (Either in or out)\n",
    "                                        mh, hotspot_2_origin_idx_2_crit_dest_idx, hotspot_flows, list_indices_all_fluxes_for_colormap = pipeline_mobility_hierarchy_time_day_type_trips(cities_gdf = geojson_input_hierarchy,\n",
    "                                                                                                                                                                                        Tij_dist_fit_gravity = Tij_dist,\n",
    "                                                                                                                                                                                        str_population_col_grid = str_population_col_grid,\n",
    "                                                                                                                                                                                        str_col_comuni_name = str_col_comuni_name,\n",
    "                                                                                                                                                                                        str_col_origin = str_col_origin,\n",
    "                                                                                                                                                                                        str_col_destination = str_col_destination,\n",
    "                                                                                                                                                                                        str_col_n_trips = str_col_n_trips,\n",
    "                                                                                                                                                                                        str_col_total_flows_grid = str_col_total_flows_grid,\n",
    "                                                                                                                                                                                        str_hotspot_prefix = str_hotspot_prefix,\n",
    "                                                                                                                                                                                        str_centroid_lat = str_centroid_lat,\n",
    "                                                                                                                                                                                        str_centroid_lon = str_centroid_lon,\n",
    "                                                                                                                                                                                        str_grid_idx = str_grid_idx,\n",
    "                                                                                                                                                                                        user_profile = user_profile,\n",
    "                                                                                                                                                                                        str_t = str_t,\n",
    "                                                                                                                                                                                        str_t1 = str_t1,\n",
    "                                                                                                                                                                                        is_in_flows = is_in_flows,                                                                                                  # NOTE: is_in_flows = True means that we are considering the incoming fluxes to the hotspot\n",
    "                                                                                                                                                                                        columns_2_hold_geopandas = None,\n",
    "                                                                                                                                                                                            int_levels = 7)\n",
    "                                        dict_output_hotspot_analysis = fill_dict_output_hotspot_analysis_OD_analysis_from_case_pipeline(\n",
    "                                                                                                            dict_output_hotspot_analysis = dict_output_hotspot_analysis,\n",
    "                                                                                                            str_day = str_day,\n",
    "                                                                                                            time_interval = time_interval,\n",
    "                                                                                                            user_profile = user_profile,\n",
    "                                                                                                            is_weekday = is_weekday,\n",
    "                                                                                                            is_in_flows = is_in_flows,\n",
    "                                                                                                            suffix_in = suffix_in,\n",
    "                                                                                                            case_pipeline = case_pipeline,\n",
    "                                                                                                            hotspot_2_origin_idx_2_crit_dest_idx = hotspot_2_origin_idx_2_crit_dest_idx,\n",
    "                                                                                                            list_indices_all_fluxes_for_colormap = list_indices_all_fluxes_for_colormap,\n",
    "                                                                                                            hotspot_flows = hotspot_flows\n",
    "                                                                                                            )   \n",
    "                                        print(f\"Map flux generated for {str_t}-{str_t1}, {user_profile}, {suffix_in} flows\")\n",
    "                                        hotspot_levels = get_values_from_case_pipeline_OD_analysis(dict_column_flows = None,dict_column_grid = None,dict_output_hotspot_analysis = dict_output_hotspot_analysis,str_day = str_day,time_interval = time_interval,user_profile = user_profile,is_weekday = is_weekday,is_in_flows = is_in_flows,suffix_in = suffix_in,name_dict = \"dict_output_hotspot_analysis\",name_key = \"hotspot_levels\",case_pipeline = case_pipeline)\n",
    "                                        # NOTE: Save the maps for incoming fluxes -> NOTE: The str_output_dir_date is unique \n",
    "                                        save_output_mobility_hierarchy_dependent_is_in_fluxes(\n",
    "                                                                                            str_dir_output_date = str_dir_output_date,\n",
    "                                                                                            map_hierarchy = mh.fmap,\n",
    "                                                                                            user_profile = user_profile,\n",
    "                                                                                            hotspot_levels = hotspot_levels,\n",
    "                                                                                            hotspot_2_origin_idx_2_crit_dest_idx = hotspot_2_origin_idx_2_crit_dest_idx,\n",
    "                                                                                            str_t = str_t,\n",
    "                                                                                            str_t1 = str_t1,\n",
    "                                                                                            is_in_flows = is_in_flows                                                                                                  # NOTE: is_in_flows = True means that we are considering the incoming fluxes to the hotspot\n",
    "                                                                                                )                                                                                                             # show the map in the browser\n",
    "                                        # Memory management: Clear map_flux after saving\n",
    "                                        gc.collect()                                        \n",
    "                                        # NOTE: Global save the flows and cities_gdf by merging the str_day, str_t, str_t1, user_profile\n",
    "                                        print(f\"Day: {str_day}, Time: {str_t}-{str_t1}, User Profile: {user_profile}, Case: {suffix_in} - mh.flows: {mh.flows.columns}\\nflows merged: {Tij_dist.columns}\\nmh.grid: {mh.grid.columns}\")\n",
    "                                        \n",
    "                                        # Ensure AREA_ID has the same type before merging\n",
    "                                        if str_area_id_presenze in global_cities_gdf_holding_all_columns_flows.columns:\n",
    "                                            global_cities_gdf_holding_all_columns_flows[str_area_id_presenze] = global_cities_gdf_holding_all_columns_flows[str_area_id_presenze].astype(str)\n",
    "                                        if str_area_id_presenze in mh.grid.columns:\n",
    "                                            mh.grid[str_area_id_presenze] = mh.grid[str_area_id_presenze].astype(str)\n",
    "\n",
    "                                        global_Tij_holding_all_columns_flows, global_cities_gdf_holding_all_columns_flows = merge_flows_and_grid_with_global_to_obtain_unique_dfs(global_Tij_holding_all_columns_flows = global_Tij_holding_all_columns_flows,\n",
    "                                                                                                                                                                                    flows_2_be_merged = Tij_dist,\n",
    "                                                                                                                                                                                    global_cities_gdf_holding_all_columns_flows = global_cities_gdf_holding_all_columns_flows,\n",
    "                                                                                                                                                                                    grid_single_case_2_be_merged = mh.grid,\n",
    "                                                                                                                                                                                    columns_join_global_geopandas = columns_2_hold_geopandas_for_flows_plot,\n",
    "                                                                                                                                                                                    columns_flows_2_be_merged_2_keep = columns_flows_2_be_merged_2_keep,\n",
    "                                                                                                                                                                                    on_columns_flows_2_join = on_colums_flows_2_join,\n",
    "                                                                                                                                                                                    on_columns_grid_2_join = on_columns_grid_2_join,\n",
    "                                                                                                                                                                                    message_geojson = f\"{str_day} {str_t}-{str_t1} {user_profile}\",\n",
    "                                                                                                                                                                                    message_flows = f\"{str_day} {str_t}-{str_t1} {user_profile}\"\n",
    "                                                                                                                                                                                    )                                    \n",
    "                                        # Memory management: Clear baseline analysis variables\n",
    "                                        gc.collect()\n",
    "                                        idx_case += 1\n",
    "                                        print(f\"Number columns global flows after join {str_t}-{str_t1} {user_profile}: \",len(global_Tij_holding_all_columns_flows.columns),f\" cities: \",len(global_cities_gdf_holding_all_columns_flows.columns))\n",
    "                                # NOTE: Visualize All Outputs\n",
    "                                for suffix_in,is_in_flows in case_2_is_in_flow.items():                                                   \n",
    "                                    # Plot!\n",
    "                                    hotspot_2_origin_idx_2_crit_dest_idx, str_col_total_flows_grid, str_col_hotspot_level, str_col_n_trips, str_caption_colormap, str_col_difference = extract_name_columns_for_hierarchical_plot(dict_column_flows = dict_column_flows,dict_column_grid = dict_column_grid,dict_output_hotspot_analysis = dict_output_hotspot_analysis,\n",
    "                                                                                                                                                                                                                                str_day = str_day,time_interval = time_interval,user_profile = user_profile,is_weekday = is_weekday,is_in_flows = is_in_flows,\n",
    "                                                                                                                                                                                                                                suffix_in = suffix_in,case_pipeline = case_pipeline)\n",
    "                                    # Map Hierarchical Flows\n",
    "                                    map_flux = visualize_critical_fluxes_with_lines(grid = global_cities_gdf_holding_all_columns_flows,\n",
    "                                                                                    df_flows = global_Tij_holding_all_columns_flows,\n",
    "                                                                                    hotspot_2_origin_idx_2_crit_dest_idx = hotspot_2_origin_idx_2_crit_dest_idx,                        # NOTE:\n",
    "                                                                                    str_col_total_flows_grid = str_col_total_flows_grid,\n",
    "                                                                                    str_col_hotspot = str_col_hotspot_level,\n",
    "                                                                                    str_col_n_trips = str_col_n_trips,\n",
    "                                                                                    is_in_flows = is_in_flows,\n",
    "                                                                                    str_col_origin = str_col_origin,\n",
    "                                                                                    str_col_destination = str_col_destination,\n",
    "                                                                                    str_centroid_lat = str_centroid_lat,\n",
    "                                                                                    str_centroid_lon = str_centroid_lon,\n",
    "                                                                                    str_col_comuni_name= str_col_comuni_name,\n",
    "                                                                                    str_grid_idx = str_grid_idx,\n",
    "                                                                                    str_caption_colormap = str_caption_colormap,\n",
    "                                                                                    str_colormap= 'YlOrRd'\n",
    "                                                    )\n",
    "                                    complete_path_map = os.path.join(str_dir_output_date,f\"map_fluxes_{user_profile}_t_{str_t}_{str_t1}_{suffix_in}.html\")                                                # path to the map file\n",
    "                                    \n",
    "                                    # Map Critical Flows\n",
    "                                    fmap_baseline = plot_negative_differences_interactive(\n",
    "                                                                                grid = geojson_input_hierarchy, \n",
    "                                                                                flows_negative = global_Tij_holding_all_columns_flows, \n",
    "                                                                                str_col_i = str_col_origin, \n",
    "                                                                                str_col_j = str_col_destination, \n",
    "                                                                                str_col_difference = str_col_difference,\n",
    "                                                                                str_centroid_lat = str_centroid_lat, \n",
    "                                                                                str_centroid_lon = str_centroid_lon, \n",
    "                                                                                caption_colorbar = f\"Excess in tourism with respect to baseline {user_profile}\"\n",
    "                                                                                )\n",
    "                                    try:\n",
    "                                        map_flux.save(complete_path_map)                                                                                                                         # save the map to the output folder\n",
    "                                    except Exception as e:\n",
    "                                        print(f\"Error saving map_flux: {e}\")\n",
    "                                    print(f\"Map critical flows:  {str_t}-{str_t1}, {user_profile}\")\n",
    "                                    fmap_baseline.save(os.path.join(str_dir_output_date, f\"most_critical_directions_{user_profile}_{str_t}_{str_t1}_{str_day}.html\"))                                                                                     # save the map in the output folder                            \n",
    "                                    del map_flux,fmap_baseline\n",
    "                                    gc.collect()\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # NOTE: Initialize Pipeline for GTFS data\n",
    "                            if is_gtfs_available:                                                                                                                         # if the gtfs is available\n",
    "                                time_vector_OD = pd.timedelta_range(start = f\"{int_hour_start_window_interest}h\",end = f\"{int_hour_end_window_interest}h\",freq = f\"{int_min_aggregation_OD}min\")\n",
    "                                str_col_n_trips_bus = f\"n_trips_bus_{str_t}_{str_t1}\"\n",
    "                                columns_2_hold_geopandas_in_bus = [str_col_n_trips_bus, str_centroid_lat, str_centroid_lon,\n",
    "                                                                    str_col_comuni_name, str_grid_idx, \"geometry\", str_col_hotspot_level_in]\n",
    "                                str_col_n_users_bus = f\"n_users_bus_{str_t}_{str_t1}\"                                                                                                 # column name for the number of users bus\n",
    "                                int_number_people_per_bus = 50                                                                                                                         # number of people per bus\n",
    "                                str_col_difference_bus = f\"difference_bus_{str_t}_{str_t1}\"                            \n",
    "                                idx_case_bus = 0\n",
    "                                for suffix_in,is_in_flow_buses in case_2_is_in_flow.items():                                                   \n",
    "                                    # ------------ Initialize Analysis of the Gtfs Data ------------- # \n",
    "                                    # NOTE: Init GTFS analysis\n",
    "                                    config[f\"{str_prefix_complete_path}_{str_name_gdf_stops}\"] = os.path.join(str_dir_output_date,f\"{str_name_gdf_stops}_{str_t}_{str_t1}.geojson\")             # full path stops (gdf): NOTE: it changes from date to date\n",
    "                                    print(f\"Initializing gdf stops: \",config[f\"{str_prefix_complete_path}_{str_name_gdf_stops}\"])\n",
    "                                    gdf_stops, trips_in_time_interval, stop_times_in_time_interval = compute_routes_trips(feed, str_day, time_vector_OD, config,\n",
    "                                                                                                                           str_prefix_complete_path,str_name_gdf_stops,str_trip_idx)\n",
    "                                    gc.collect()                                \n",
    "                                    # NOTE: Associate Gtfs trips available in the Grid (shape of the city) -> helps estimate the number of trips that go from O -> D\n",
    "                                    geojson_input_hierarchy, config, grid_idx_2_route_idx, stop_id_2_trip_id, stop_id_2_route_id, grid_idx_2_stop_idx, stop_idx_2_grid_idx, name_stop_idx_2_grid_idx = pipeline_associate_stops_trips_routes_2_grid(config,\n",
    "                                                                                                                                                                                                                                    stop_times_in_time_interval,\n",
    "                                                                                                                                                                                                                                    trips_in_time_interval,\n",
    "                                                                                                                                                                                                                                    gdf_stops,\n",
    "                                                                                                                                                                                                                                    geojson_input_hierarchy,\n",
    "                                                                                                                                                                                                                                    str_grid_idx, str_stop_idx, str_trip_idx, str_route_idx,\n",
    "                                                                                                                                                                                                                                    str_name_stop_id,\n",
    "                                                                                                                                                                                                                                    type_grid_idx = type_grid_idx,\n",
    "                                                                                                                                                                                                                                    type_stop_idx = type_stop_idx,\n",
    "                                                                                                                                                                                                                                    type_trip_idx = type_trip_idx,\n",
    "                                                                                                                                                                                                                                    type_route_idx = type_route_idx,                                                                                                                                                                                                                    \n",
    "                                                                                                                                                                                                                                    str_col_n_stops = f\"{str_col_n_stops}_{str_t}_{str_t1}\",\n",
    "                                                                                                                                                                                                                                    str_prefix_complete_path = str_prefix_complete_path,\n",
    "                                                                                                                                                                                                                                    str_dir_output_date = str_dir_output_date,\n",
    "                                                                                                                                                                                                                                    str_name_stop_2_trip = str_name_stop_2_trip,\n",
    "                                                                                                                                                                                                                                    str_name_stop_2_route = str_name_stop_2_route,\n",
    "                                                                                                                                                                                                                                    str_name_grid_2_stop = str_name_grid_2_stop,\n",
    "                                                                                                                                                                                                                                    str_name_stop_2_grid = str_name_stop_2_grid,\n",
    "                                                                                                                                                                                                                                    str_name_name_stop_2_grid = str_name_name_stop_2_grid,\n",
    "                                                                                                                                                                                                                                    str_name_grid_2_route = str_name_grid_2_route,)                \n",
    "                                    \n",
    "                                    # NOTE: Add columns bus trip -> differ from OD fluxes\n",
    "                                    mh.flows = pipeline_associate_route_trips_2_flows(mh.flows,\n",
    "                                                                                    grid_idx_2_route_idx,\n",
    "                                                                                    str_col_origin,\n",
    "                                                                                    str_col_destination,\n",
    "                                                                                    str_col_n_trips_bus)\n",
    "                                    # NOTE: Mobility Hierarchy Analysis - Inflows and Outflows for Bus Trips\n",
    "                                    mh_bus, map_flux_bus,hotspot_2_origin_idx_2_crit_dest_idx_bus, hotspot_in_flows_bus,list_indices_all_fluxes_for_colormap = pipeline_mobility_hierarchy_time_day_type_trips(geojson_input_hierarchy,\n",
    "                                                                                                                                                    pl.DataFrame(mh.flows),\n",
    "                                                                                                                                                    str_population_col_grid,\n",
    "                                                                                                                                                    str_col_comuni_name,                                    # NOTE: Must be present in flows -> index geometry name: str   \n",
    "                                                                                                                                                    str_col_origin,                                         # NOTE: Must be present in flows -> index geometry origin: int  \n",
    "                                                                                                                                                    str_col_destination,                                    # NOTE: Must be present in flows -> index geometry destination: int\n",
    "                                                                                                                                                    str_col_n_trips_bus,                                    # NOTE: Must be present in flows -> numer of trips: int\n",
    "                                                                                                                                                    str_col_total_in_flows_grid,                            # NOTE: This column is created inside the function\n",
    "                                                                                                                                                    str_col_total_out_flows_grid,                           # NOTE: This column is created inside the function\n",
    "                                                                                                                                                    str_hotspot_prefix,\n",
    "                                                                                                                                                    str_centroid_lat,\n",
    "                                                                                                                                                    str_centroid_lon,\n",
    "                                                                                                                                                    str_grid_idx,\n",
    "                                                                                                                                                    \"bus\",\n",
    "                                                                                                                                                    str_t,\n",
    "                                                                                                                                                    str_t1,\n",
    "                                                                                                                                                    is_in_flows = is_in_flow_buses,                                                                                                  # NOTE: is_in_flows = True means that we are considering the incoming fluxes to the hotspot\n",
    "                                                                                                                                                    columns_2_hold_geopandas = columns_2_hold_geopandas_in_bus,\n",
    "                                                                                                                                                    int_levels = 5)                                \n",
    "                                    # NOTE: Save the output of the bus hierarchy analysis for Buses\n",
    "                                    save_output_mobility_hierarchy_dependent_is_in_fluxes(                                   \n",
    "                                                                                            str_dir_output_date = str_dir_output_date,\n",
    "                                                                                            map_flux = map_flux_bus,\n",
    "                                                                                            map_hierarchy = mh_bus.fmap,\n",
    "                                                                                            user_profile = \"bus\",\n",
    "                                                                                            hotspot_levels = hotspot_in_flows_bus,\n",
    "                                                                                            hotspot_2_origin_idx_2_crit_dest_idx = hotspot_2_origin_idx_2_crit_dest_idx_bus,\n",
    "                                                                                            str_t = str_t,\n",
    "                                                                                            str_t1 = str_t1,\n",
    "                                                                                            is_in_flows = is_in_flow_buses                                                                                                  # NOTE: is_in_flows = True means that we are considering the incoming fluxes to the hotspot\n",
    "                                                                                                )  \n",
    "                   \n",
    "                                    if SAVE_SINGLE_GEOSPATIAL_FILES:\n",
    "                                        save_output_hierarchy_analysis(config = config,\n",
    "                                                                    str_dir_output_date = str_dir_output_date,\n",
    "                                                                    flows = mh_bus.flows,\n",
    "                                                                    grid = mh_bus.grid[columns_2_hold_geopandas],\n",
    "                                                                    user_profile = \"bus\",\n",
    "                                                                    str_t = str_t,\n",
    "                                                                    str_t1 = str_t1,\n",
    "                                                                        )                \n",
    "\n",
    "\n",
    "                                    # NOTE: Here compute the difference between the number of trips bus and \n",
    "                                    # the number of trips in the flows                                    \n",
    "                                    mh_bus.flows = mh_bus.flows.with_columns((pl.col(str_col_n_trips_bus)* int_number_people_per_bus).alias(str_col_n_users_bus))                                                                 # compute the number of users bus\n",
    "                                    # Compute difference and filter\n",
    "                                    # If mh.flows and mh_bus.flows are Pandas DataFrames:\n",
    "                                    flows_merged_bus = mh.flows.merge(\n",
    "                                                                mh_bus.flows.to_pandas()[[str_col_origin, str_col_destination, str_col_n_users_bus]],\n",
    "                                                                on=[str_col_origin, str_col_destination],\n",
    "                                                                how=\"left\"\n",
    "                                                            )\n",
    "                                    flows_merged_bus = pl.DataFrame(flows_merged_bus)\n",
    "                                    # NOTE: Compute the difference between the number of users bus and the number of trips (from OD) in the flows\n",
    "                                    flows_merged_bus = flows_merged_bus.with_columns(\n",
    "                                        (pl.col(str_col_n_users_bus) - pl.col(str_col_n_trips)).alias(str_col_difference_bus)\n",
    "                                    )\n",
    "                                    # NOTE: Filter the flows with negative differences (The buses fail to cover the demand at least of 10 people)\n",
    "                                    flows_negative_bus = flows_merged_bus.filter(pl.col(str_col_difference_bus) < - 10)\n",
    "\n",
    "                                    # Plot\n",
    "                                    fmap = plot_negative_differences_interactive(\n",
    "                                                                                mh.grid, \n",
    "                                                                                flows_negative_bus, \n",
    "                                                                                str_col_origin, \n",
    "                                                                                str_col_destination, \n",
    "                                                                                str_col_difference_bus,\n",
    "                                                                                str_centroid_lat, \n",
    "                                                                                str_centroid_lon, \n",
    "                                                                                title=\"Direction with need for Bus Supply\"\n",
    "                                                                                )\n",
    "                                    fmap.save(os.path.join(str_dir_output_date, f\"need_for_bus_{str_t}_{str_t1}_{str_day}.html\"))                                                                                     # save the map in the output folder\n",
    "                                    # Save the map                                  \n",
    "                                    with open(os.path.join(str_dir_output_date, f\"config_{str_t}_{str_t1}.json\"), \"w\") as f:\n",
    "                                        json.dump(config, f, indent=4)                                                                                     # save the config file with the current configuration\n",
    "                                    idx_case_bus += 1\n",
    "                                    global_Tij_holding_all_columns_flows, global_cities_gdf_holding_all_columns_flows = merge_flows_and_grid_with_global_to_obtain_unique_dfs(global_Tij_holding_all_columns_flows = global_Tij_holding_all_columns_flows,\n",
    "                                                                                                                                                                                flows_2_be_merged = flows_merged,\n",
    "                                                                                                                                                                                global_cities_gdf_holding_all_columns_flows = global_cities_gdf_holding_all_columns_flows,\n",
    "                                                                                                                                                                                grid_single_case_2_be_merged = mh.grid,\n",
    "                                                                                                                                                                                columns_join_global_geopandas = columns_join_global_geopandas,\n",
    "                                                                                                                                                                                columns_flows_2_be_merged_2_keep = columns_flows_2_be_merged_2_keep,\n",
    "                                                                                                                                                                                on_columns_flows_2_join = on_colums_flows_2_join,\n",
    "                                                                                                                                                                                on_columns_grid_2_join = [str_grid_idx],\n",
    "                                                                                                                                                                                message_geojson = f\"{str_day} {str_t}-{str_t1} {user_profile}\",\n",
    "                                                                                                                                                                                message_flows = f\"{str_day} {str_t}-{str_t1} {user_profile}\",\n",
    "                                                                                                                                                                                is_join_flows = True,\n",
    "                                                                                                                                                                                is_join_grid = True\n",
    "                                                                                                                                                                                )\n",
    "                                    print(f\"Number columns global flows after join {str_t}-{str_t1} bus: \",len(global_Tij_holding_all_columns_flows.columns),f\" cities: \",len(global_cities_gdf_holding_all_columns_flows.columns))                                    \n",
    "                                    \n",
    "                                    # Memory management: Clear bus analysis variables\n",
    "                                    del flows_merged_bus, flows_negative_bus, fmap, str_col_difference_bus\n",
    "                                    del gdf_stops, trips_in_time_interval, stop_times_in_time_interval\n",
    "                                    del grid_idx_2_route_idx, stop_id_2_trip_id, stop_id_2_route_id\n",
    "                                    del grid_idx_2_stop_idx, stop_idx_2_grid_idx, name_stop_idx_2_grid_idx\n",
    "                                    del mh_bus, map_flux_bus\n",
    "                                    del hotspot_2_origin_idx_2_crit_dest_idx_bus, hotspot_in_flows_bus\n",
    "                                    gc.collect()\n",
    "\n",
    "                                    # TODO: Save the routes that needs to be empowered from this criterion\n",
    "                                    # NOTE: use grid_2_route_idx to get the routes to choose them and create a dictionary with the number of trips to be added.\n",
    "                                    # NOTE: The number of trips to be added is the absolute value of the difference, so we can use the negative values to understand how many trips are needed.\n",
    "                            \n",
    "                            # Memory management: Clear large objects at the end of time interval processing\n",
    "                            if 'mh' in locals(): \n",
    "                                del mh\n",
    "                            if 'Tij_dist' in locals():\n",
    "                                del Tij_dist\n",
    "                            if 'Tij_dist_baseline' in locals():\n",
    "                                del Tij_dist_baseline\n",
    "                            if 'hotspot_2_origin_idx_2_crit_dest_idx' in locals():\n",
    "                                del hotspot_2_origin_idx_2_crit_dest_idx\n",
    "                            if 'hotspot_in_flows' in locals():\n",
    "                                del hotspot_in_flows\n",
    "                            \n",
    "                            # Close any remaining matplotlib figures\n",
    "                            safe_close_figures()\n",
    "                            gc.collect()\n",
    "\n",
    "                        global_cities_gdf_holding_all_columns_flows.to_file(os.path.join(str_dir_output_date, f\"global_cities_gdf_holding_all_columns_flows_{str_day}.geojson\"))\n",
    "                        global_Tij_holding_all_columns_flows.write_parquet(os.path.join(str_dir_output_date, f\"global_Tij_holding_all_columns_flows_{str_day}.parquet\"))\n",
    "                    # Memory management: Clear user profile specific variables\n",
    "                    safe_close_figures()  # Close any figures from visualization\n",
    "                    gc.collect()\n",
    "            \n",
    "                # Memory management: Clear time interval variables  \n",
    "                if 'time_vector_OD' in locals() and time_vector_OD is not None:\n",
    "                    del time_vector_OD\n",
    "                if 'str_time_vector' in locals() and str_time_vector is not None:\n",
    "                    del str_time_vector\n",
    "                gc.collect()\n",
    "                \n",
    "                # Memory management: Clear day-specific variables\n",
    "            gc.collect()\n",
    "        \n",
    "        # Memory management: Clear file processing variables\n",
    "        if 'df_presenze' in locals() and df_presenze is not None:\n",
    "            del df_presenze\n",
    "        if 'df_od' in locals() and df_od is not None:\n",
    "            del df_od  \n",
    "        gc.collect()\n",
    "\n",
    "# Final memory management: Clear remaining global variables\n",
    "print(\"Performing final memory cleanup...\")\n",
    "if 'cities_gdf' in locals():\n",
    "    del cities_gdf\n",
    "if 'df_distance_matrix' in locals():\n",
    "    del df_distance_matrix\n",
    "if 'direction_matrix' in locals():\n",
    "    del direction_matrix\n",
    "if 'distance_matrix' in locals():\n",
    "    del distance_matrix\n",
    "if 'Istat_obj' in locals():\n",
    "    del Istat_obj\n",
    "if 'feed' in locals():\n",
    "    del feed\n",
    "if 'df_presenze_null_days' in locals():\n",
    "    del df_presenze_null_days\n",
    "if 'df_od_null_days' in locals():\n",
    "    del df_od_null_days\n",
    "if 'data_handler' in locals():\n",
    "    del data_handler\n",
    "\n",
    "# Final garbage collection\n",
    "gc.collect()\n",
    "print(\"Memory cleanup completed.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bf0b56de",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "str_col_n_trips = dict_column_flows[str_day][time_interval[0]][user_profile][is_weekday][suffix_in][f\"str_col_n_trips\"]\n",
    "str_col_total_flows_grid = dict_column_grid[str_day][time_interval[0]][user_profile][is_weekday][suffix_in][\"str_col_total_flows_grid_hierachical_routine\"]\n",
    "str_caption_colormap = dict_column_flows[str_day][time_interval[0]][user_profile][is_weekday][suffix_in][\"str_caption_colormap_flows\"]\n",
    "mask = mh.grid[str_col_total_flows_grid] > 0\n",
    "grid = mh.grid[mask]\n",
    "df_flows = Tij_dist.filter(pl.col(str_col_n_trips) > 0)\n",
    "print(f\" - n grid after dropping NaN total flows: {len(grid)}\")\n",
    "# Init Map\n",
    "center_lat = grid[str_centroid_lat].mean()\n",
    "center_lon = grid[str_centroid_lon].mean()\n",
    "fmap = folium.Map(location=[center_lat, center_lon], zoom_start=10, tiles='cartodbpositron')    \n",
    "grid_idx_2_grid_name = dict(zip(grid[str_grid_idx], grid[str_col_comuni_name]))\n",
    "# Flatten the nested list structure for Polars filtering\n",
    "flat_indices = []\n",
    "for indices_list in list_indices_all_fluxes_for_colormap:\n",
    "    if isinstance(indices_list, (list, tuple, np.ndarray)):\n",
    "        flat_indices.extend(indices_list)\n",
    "    else:\n",
    "        flat_indices.append(indices_list)\n",
    "\n",
    "# Remove duplicates and ensure we have a clean list of integers\n",
    "flat_indices = list(set(int(idx) for idx in flat_indices if idx is not None))\n",
    "\n",
    "# Filter flows based on direction for colormap computation\n",
    "if is_in_flows:\n",
    "    # Filter flows where destination is in our list of flux indices\n",
    "    filtered_flows = df_flows.filter(pl.col(str_col_destination).is_in(flat_indices))\n",
    "else:\n",
    "    # Filter flows where origin is in our list of flux indices\n",
    "    filtered_flows = df_flows.filter(pl.col(str_col_origin).is_in(flat_indices))\n",
    "\n",
    "# Create colormap based on number of trips\n",
    "colormap, min_trips, max_trips = get_colormap_from_df_fluxes_and_col(\n",
    "    filtered_flows,\n",
    "    str_col_n_trips,\n",
    "    caption=str_caption_colormap, \n",
    "    colormap=str_colormap\n",
    ")    \n",
    "# --- Tooltip fields logic ---\n",
    "tooltip_fields = []\n",
    "tooltip_aliases = []\n",
    "if str_col_comuni_name in grid.columns:\n",
    "    tooltip_fields.append(str_col_comuni_name)\n",
    "    tooltip_aliases.append('City:')\n",
    "if str_col_total_flows_grid and str_col_total_flows_grid in grid.columns:\n",
    "    tooltip_fields.append(str_col_total_flows_grid)\n",
    "    tooltip_aliases.append('Total Flows:')\n",
    "\n",
    "# Add base grid layer (lightly colored)\n",
    "folium.GeoJson(\n",
    "    grid,\n",
    "    name=\"Grid\",\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': 'lightgray',\n",
    "        'color': 'gray',\n",
    "        'weight': 1,\n",
    "        'fillOpacity': 0.2,\n",
    "        'opacity': 0.5\n",
    "    }#,\n",
    "#        tooltip=folium.GeoJsonTooltip(\n",
    "#            fields=tooltip_fields,\n",
    "#            aliases=tooltip_aliases,\n",
    "#            localize=True\n",
    "#       )\n",
    ").add_to(fmap)\n",
    "    \n",
    "# Create a mapping from grid index to coordinates\n",
    "grid_coords = {}\n",
    "for idx, row in grid.iterrows():\n",
    "    grid_coords[idx] = [row[str_centroid_lat], row[str_centroid_lon]]\n",
    "\n",
    "# Iterate through levels and draw fluxes\n",
    "for level, origin_idx_2_crit_dest_idx in hotspot_2_origin_idx_2_crit_dest_idx.items():\n",
    "    # Create a feature group for this level\n",
    "    level_group = folium.FeatureGroup(name=f\"Level {level} Fluxes\")\n",
    "    \n",
    "    if is_in_flows:\n",
    "        # For incoming flows: idx_dest_flux is destination, idces_orig_fluxe are origins\n",
    "        for idx_dest_flux, idces_orig_fluxe in origin_idx_2_crit_dest_idx.items():\n",
    "            # Highlight the destination hotspot\n",
    "            if idx_dest_flux in grid_coords:\n",
    "                dest_coords = grid_coords[idx_dest_flux]\n",
    "                folium.CircleMarker(\n",
    "                    location=dest_coords,\n",
    "                    radius=10,\n",
    "                    color='red',\n",
    "                    fillColor='darkred',\n",
    "                    fillOpacity=0.8,\n",
    "                    popup=folium.Popup(\n",
    "                        f\"<b>Destination Hotspot</b><br>\"\n",
    "                        f\"Level: {level}<br>\"\n",
    "                        f\"Index: {grid_idx_2_grid_name[idx_dest_flux]}<br>\"\n",
    "                        f\"# Regions Incoming flow: {len(idces_orig_fluxe)}\"\n",
    "                    )\n",
    "                ).add_to(level_group)\n",
    "            \n",
    "            # Draw lines from each origin to this destination\n",
    "            for idx_orig_flux in idces_orig_fluxe:\n",
    "                # Find the specific flow data\n",
    "                flow_data = df_flows.filter(\n",
    "                    (pl.col(str_col_origin) == idx_orig_flux) & \n",
    "                    (pl.col(str_col_destination) == idx_dest_flux)\n",
    "                )\n",
    "                \n",
    "                if len(flow_data) > 0:\n",
    "                    n_trips = flow_data[str_col_n_trips].item()\n",
    "                    \n",
    "                    # Draw the flux line\n",
    "                    level_group = draw_flux_line(\n",
    "                        idx_orig_flux,\n",
    "                        idx_dest_flux, \n",
    "                        level, \n",
    "                        n_trips,\n",
    "                        grid_coords,\n",
    "                        min_trips,\n",
    "                        max_trips,\n",
    "                        level_group,\n",
    "                        is_in_flows,\n",
    "                        grid_idx_2_grid_name,\n",
    "                        colormap\n",
    "                    )\n",
    "                \n",
    "                    # Add origin marker\n",
    "                    if idx_orig_flux in grid_coords:\n",
    "                        origin_coords = grid_coords[idx_orig_flux]\n",
    "                        folium.CircleMarker(\n",
    "                            location=origin_coords,\n",
    "                            radius=4,\n",
    "                            color='blue',\n",
    "                            fillColor='lightblue',\n",
    "                            fillOpacity=0.6,\n",
    "                            popup=folium.Popup(\n",
    "                                f\"<b>Origin Point</b><br>\"\n",
    "                                f\"Index: {idx_orig_flux}<br>\"\n",
    "                                f\"Contributes to Level {level} hotspot\"\n",
    "                            )\n",
    "                        ).add_to(level_group)\n",
    "    \n",
    "    else:\n",
    "        # For outgoing flows: idx_orig_flux is origin, idces_dest_fluxe are destinations\n",
    "        for idx_orig_flux, idces_dest_fluxe in origin_idx_2_crit_dest_idx.items():\n",
    "            # Highlight the origin hotspot\n",
    "            if idx_orig_flux in grid_coords:\n",
    "                origin_coords = grid_coords[idx_orig_flux]\n",
    "                folium.CircleMarker(\n",
    "                    location=origin_coords,\n",
    "                    radius=10,\n",
    "                    color='green',\n",
    "                    fillColor='darkgreen',\n",
    "                    fillOpacity=0.8,\n",
    "                    popup=folium.Popup(\n",
    "                        f\"<b>Origin Hotspot</b><br>\"\n",
    "                        f\"Level: {level}<br>\"\n",
    "                        f\"Index: {idx_orig_flux}<br>\"\n",
    "                        f\"# Regions Outgoing flows to {len(idces_dest_fluxe)} destinations\"\n",
    "                    )\n",
    "                ).add_to(level_group)\n",
    "            \n",
    "            # Draw lines from this origin to each destination\n",
    "            for idx_dest_flux in idces_dest_fluxe:\n",
    "                # Find the specific flow data\n",
    "                flow_data = df_flows.filter(\n",
    "                    (pl.col(str_col_origin) == idx_orig_flux) & \n",
    "                    (pl.col(str_col_destination) == idx_dest_flux)\n",
    "                )\n",
    "                \n",
    "                if len(flow_data) > 0:\n",
    "                    n_trips = flow_data[str_col_n_trips].item()\n",
    "                    \n",
    "                    # Draw the flux line\n",
    "                    level_group = draw_flux_line(\n",
    "                        idx_orig_flux,\n",
    "                        idx_dest_flux, \n",
    "                        level, \n",
    "                        n_trips,\n",
    "                        grid_coords,\n",
    "                        min_trips,\n",
    "                        max_trips,\n",
    "                        level_group,\n",
    "                        is_in_flows,\n",
    "                        grid_idx_2_grid_name,\n",
    "                        colormap\n",
    "                    )\n",
    "                \n",
    "                    # Add destination marker\n",
    "                    if idx_dest_flux in grid_coords:\n",
    "                        dest_coords = grid_coords[idx_dest_flux]\n",
    "                        folium.CircleMarker(\n",
    "                            location=dest_coords,\n",
    "                            radius=4,\n",
    "                            color='orange',\n",
    "                            fillColor='lightyellow',\n",
    "                            fillOpacity=0.6,\n",
    "                            popup=folium.Popup(\n",
    "                                f\"<b>Destination Point</b><br>\"\n",
    "                                f\"Index: {idx_dest_flux}<br>\"\n",
    "                                f\"Receives from Level {level} hotspot\"\n",
    "                            )\n",
    "                        ).add_to(level_group)\n",
    "    \n",
    "    # Add the level group to the map\n",
    "    level_group.add_to(fmap)\n",
    "\n",
    "# Add the colormap legend\n",
    "colormap.add_to(fmap)\n",
    "\n",
    "# Add layer control\n",
    "folium.LayerControl().add_to(fmap)\n",
    "fmap"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2ed2d82e",
   "metadata": {},
   "source": [
    "# Linking Geography and Network"
   ]
  },
  {
   "cell_type": "code",
   "id": "da6fd911",
   "metadata": {},
   "source": [
    "from network_analysis import build_nk_graph_from_hotspot_dict, plot_nk_graph_geometrical_interactive\n",
    "G,edges_features = build_nk_graph_from_hotspot_dict(hotspot_2_origin_idx_2_crit_dest_idx,\n",
    "                                    mh.grid,\n",
    "                                    mh.flows,\n",
    "                                    str_col_i = str_col_origin,\n",
    "                                    str_col_j = str_col_destination, \n",
    "                                    str_col_weight = str_col_n_trips)\n",
    "\n",
    "plot_nk_graph_geometrical_interactive(G,  \n",
    "                                      mh.grid, \n",
    "                                      str_centroid_lat, \n",
    "                                      str_centroid_lon)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ca26a738",
   "metadata": {},
   "source": [
    "import networkx as nx\n",
    "import networkit as nk\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nxG = nk.nxadapter.nk2nx(G)  # convert from NetworKit.Graph to networkx.Graph\n",
    "A = nx.to_numpy_array(nxG, weight='weight')   # Get the weighted adjacency matrix\n",
    "\n",
    "sns.heatmap(A, cmap='viridis', cbar=True, square=True, annot=False, fmt='.1f')\n",
    "plt.xlabel(\"Destination Node\")\n",
    "plt.ylabel(\"Origin Node\")\n",
    "plt.title(\"Weighted Adjacency Matrix Heatmap\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "855e5421",
   "metadata": {},
   "source": [
    "# Eigenvector centrality\n",
    "ec = nk.centrality.EigenvectorCentrality(G)\n",
    "ec.run()\n",
    "ec.ranking()[:10] # the 10 most central nodes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "28e19ef2",
   "metadata": {},
   "source": [
    "# Fluxes Hierarchy (Plot Hotspots)"
   ]
  },
  {
   "cell_type": "code",
   "id": "89085c7d",
   "metadata": {},
   "source": [
    "# Create a heatmap of the bus trips matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def plot_heatmap_flows(flows_df, \n",
    "                       str_col_origin, \n",
    "                       str_col_destination, \n",
    "                       str_col_n_trips_bus, \n",
    "                       cbar_kws,\n",
    "                       title,\n",
    "                       save_path=None):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of the bus trips matrix.\n",
    "    \"\"\"\n",
    "    # Create a pivot table to transform the data into a matrix format\n",
    "    bus_trips_matrix = flows_df.pivot(index=str_col_origin, \n",
    "                                      columns=str_col_destination, \n",
    "                                      values=str_col_n_trips_bus)\n",
    "\n",
    "    # Fill NaN values with 0 (if there are missing combinations)\n",
    "    bus_trips_matrix = bus_trips_matrix.fillna(0)\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(bus_trips_matrix, \n",
    "                annot=True,  # Show values in cells\n",
    "                fmt='g',     # Format for numbers\n",
    "                cmap='YlOrRd',  # Color scheme\n",
    "                cbar_kws={'label': cbar_kws},\n",
    "                square=True)  # Make cells square\n",
    "\n",
    "    plt.title(title, \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Destination Grid Cell (j)', fontsize=12)\n",
    "    plt.ylabel('Origin Grid Cell (i)', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Print some statistics about the matrix\n",
    "    print(f\"Matrix shape: {bus_trips_matrix.shape}\")\n",
    "    print(f\"Total non-zero connections: {(bus_trips_matrix > 0).sum().sum()}\")\n",
    "    print(f\"Maximum number of common routes: {bus_trips_matrix.max().max()}\")\n",
    "    print(f\"Average number of common routes (non-zero): {bus_trips_matrix[bus_trips_matrix > 0].mean().mean():.2f}\")\n",
    "    print(f\"Fraction 2 complete graph: \",(bus_trips_matrix > 0).sum().sum()/(len(bus_trips_matrix)*(len(bus_trips_matrix)-1)/2))\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Heatmap saved to {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "cbar_kws = 'Number of Common Bus Routes'\n",
    "title = f'Bus Route Intersections Matrix\\n{user_profile} - {str_t} to {str_t1}'\n",
    "plot_heatmap_flows(mh.flows, \n",
    "                       str_col_origin, \n",
    "                       str_col_destination, \n",
    "                       str_col_n_trips_bus, \n",
    "                       cbar_kws,\n",
    "                       title,\n",
    "                       save_path=None)\n",
    "cbar_kws = f'Number trips {user_profile}'\n",
    "title = f'Number trips \\n{user_profile} - {str_t} to {str_t1}'\n",
    "plot_heatmap_flows(mh.flows, \n",
    "                       str_col_origin, \n",
    "                       str_col_destination, \n",
    "                       str_col_n_trips, \n",
    "                       cbar_kws,\n",
    "                       title,\n",
    "                       save_path=None)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9b380205",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from matplotlib.colors import LogNorm\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "\n",
    "def create_daily_od_visualizations_with_animations(df_od, IndexVodafone2UserProfile, base_dir_output, \n",
    "                                                  str_period_id_presenze=\"PERIOD_ID\", top_n=10):\n",
    "    \"\"\"\n",
    "    Create and save visualizations AND animations for each day in the dataset.\n",
    "    Generates both static plots and animations for each day.\n",
    "    \n",
    "    Parameters:\n",
    "    - df_od: DataFrame with OD data\n",
    "    - IndexVodafone2UserProfile: Dictionary mapping indices to user profiles\n",
    "    - base_dir_output: Base directory to save outputs\n",
    "    - str_period_id_presenze: Column name for period/date information\n",
    "    - top_n: Number of top O-D pairs to show in bar charts\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to Polars if it's pandas\n",
    "    if isinstance(df_od, pd.DataFrame):\n",
    "        df_od = pl.from_pandas(df_od)\n",
    "    \n",
    "    # Extract date information and create str_day column\n",
    "    df_od_with_dates = df_od.with_columns([\n",
    "        pl.col(str_period_id_presenze).map_elements(\n",
    "            lambda x: extract_date_info(x)[0], \n",
    "            return_dtype=pl.Utf8\n",
    "        ).alias(\"str_day\")\n",
    "    ])\n",
    "    \n",
    "    # Get unique days and hours\n",
    "    unique_days = sorted(df_od_with_dates[\"str_day\"].unique().to_list())\n",
    "    unique_hours = sorted(df_od_with_dates[\"DEPARTURE_HOUR\"].unique().to_list())\n",
    "    \n",
    "    print(f\"Processing {len(unique_days)} days and {len(unique_hours)} hours\")\n",
    "    print(f\"Days: {unique_days}\")\n",
    "    print(f\"Hours: {unique_hours}\")\n",
    "    \n",
    "    # Create output directories\n",
    "    Path(base_dir_output).mkdir(parents=True, exist_ok=True)\n",
    "    matrix_dir = os.path.join(base_dir_output, \"matrix_plots\")\n",
    "    bar_dir = os.path.join(base_dir_output, \"bar_plots\")\n",
    "    animation_dir = os.path.join(base_dir_output, \"animations\")\n",
    "    data_dir = os.path.join(base_dir_output, \"data\")\n",
    "    \n",
    "    for dir_path in [matrix_dir, bar_dir, animation_dir, data_dir]:\n",
    "        Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get visitor classes (excluding AGGREGATED)\n",
    "    visitor_classes = [idx for idx in IndexVodafone2UserProfile.keys() if idx <= 4]\n",
    "    x_labels = [f\"D_{IndexVodafone2UserProfile[idx]}\" for idx in visitor_classes]\n",
    "    y_labels = [f\"O_{IndexVodafone2UserProfile[idx]}\" for idx in visitor_classes]\n",
    "    \n",
    "    # Get overall top O-D pairs for bar charts\n",
    "    overall_aggregated = df_od_with_dates.group_by([\n",
    "        \"O_VISITOR_CLASS_ID\", \n",
    "        \"D_VISITOR_CLASS_ID\"\n",
    "    ]).agg([\n",
    "        pl.col(\"TRIPS\").sum().alias(\"total_trips\")\n",
    "    ]).sort(\"total_trips\", descending=True)\n",
    "    \n",
    "    top_pairs = overall_aggregated.head(top_n).to_pandas()\n",
    "    top_pair_labels = []\n",
    "    top_pair_keys = []\n",
    "    \n",
    "    for _, row in top_pairs.iterrows():\n",
    "        o_class = int(row['O_VISITOR_CLASS_ID'])\n",
    "        d_class = int(row['D_VISITOR_CLASS_ID'])\n",
    "        if o_class in IndexVodafone2UserProfile and d_class in IndexVodafone2UserProfile:\n",
    "            label = f\"O_{IndexVodafone2UserProfile[o_class]} → D_{IndexVodafone2UserProfile[d_class]}\"\n",
    "            top_pair_labels.append(label)\n",
    "            top_pair_keys.append((o_class, d_class))\n",
    "    \n",
    "    # Store all data for saving\n",
    "    all_matrix_data = []\n",
    "    all_bar_data = []\n",
    "    \n",
    "    # Process each day\n",
    "    for day in unique_days:\n",
    "        print(f\"Processing day: {day}\")\n",
    "        \n",
    "        day_data = df_od_with_dates.filter(pl.col(\"str_day\") == day)\n",
    "        \n",
    "        if day_data.height == 0:\n",
    "            print(f\"No data for day {day}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare data structures for this day\n",
    "        daily_matrices = []\n",
    "        daily_bar_data = []\n",
    "        max_trips_matrix = 0\n",
    "        max_trips_bar = 0\n",
    "        \n",
    "        # Process each hour for this day\n",
    "        for hour in unique_hours:\n",
    "            print(f\"  Processing hour: {hour}\")\n",
    "            \n",
    "            # Filter data for this day and hour\n",
    "            day_hour_data = day_data.filter(pl.col(\"DEPARTURE_HOUR\") == hour)\n",
    "            \n",
    "            if day_hour_data.height == 0:\n",
    "                # Create empty data structures for missing combinations\n",
    "                matrix = np.zeros((4, 4))\n",
    "                trips_array = np.zeros(len(top_pair_keys))\n",
    "            else:\n",
    "                # === MATRIX DATA PROCESSING ===\n",
    "                # Group by origin and destination visitor classes and sum trips\n",
    "                aggregated = day_hour_data.group_by([\n",
    "                    \"O_VISITOR_CLASS_ID\", \n",
    "                    \"D_VISITOR_CLASS_ID\"\n",
    "                ]).agg([\n",
    "                    pl.col(\"TRIPS\").sum().alias(\"total_trips\")\n",
    "                ])\n",
    "                \n",
    "                aggregated_pd = aggregated.to_pandas()\n",
    "                \n",
    "                # Create matrix (4x4 for the 4 main visitor classes)\n",
    "                matrix = np.zeros((4, 4))\n",
    "                \n",
    "                for _, row in aggregated_pd.iterrows():\n",
    "                    o_class = int(row['O_VISITOR_CLASS_ID']) - 1  # Convert to 0-based indexing\n",
    "                    d_class = int(row['D_VISITOR_CLASS_ID']) - 1  # Convert to 0-based indexing\n",
    "                    \n",
    "                    # Only include valid classes (1-4, converted to 0-3)\n",
    "                    if 0 <= o_class < 4 and 0 <= d_class < 4:\n",
    "                        matrix[o_class, d_class] = row['total_trips']\n",
    "                \n",
    "                # === BAR DATA PROCESSING ===\n",
    "                trips_array = np.zeros(len(top_pair_keys))\n",
    "                \n",
    "                for _, row in aggregated_pd.iterrows():\n",
    "                    o_class = int(row['O_VISITOR_CLASS_ID'])\n",
    "                    d_class = int(row['D_VISITOR_CLASS_ID'])\n",
    "                    \n",
    "                    if (o_class, d_class) in top_pair_keys:\n",
    "                        idx = top_pair_keys.index((o_class, d_class))\n",
    "                        trips_array[idx] = row['total_trips']\n",
    "            \n",
    "            # Store data for animations\n",
    "            daily_matrices.append(matrix)\n",
    "            daily_bar_data.append(trips_array)\n",
    "            max_trips_matrix = max(max_trips_matrix, matrix.max())\n",
    "            max_trips_bar = max(max_trips_bar, trips_array.max())\n",
    "            \n",
    "            # === SAVE STATIC PLOTS ===\n",
    "            create_and_save_matrix_plot(matrix, day, hour, x_labels, y_labels, \n",
    "                                      matrix_dir, IndexVodafone2UserProfile)\n",
    "            \n",
    "            create_and_save_bar_plot(trips_array, day, hour, top_pair_labels, \n",
    "                                    bar_dir, top_n)\n",
    "            \n",
    "            # === STORE DATA FOR PARQUET FILES ===\n",
    "            # Store matrix data\n",
    "            for i in range(4):\n",
    "                for j in range(4):\n",
    "                    if matrix[i, j] > 0:  # Only store non-zero values\n",
    "                        all_matrix_data.append({\n",
    "                            'day': day,\n",
    "                            'hour': hour,\n",
    "                            'origin_class': i + 1,  # Convert back to 1-based indexing\n",
    "                            'destination_class': j + 1,\n",
    "                            'origin_class_name': IndexVodafone2UserProfile[i + 1],\n",
    "                            'destination_class_name': IndexVodafone2UserProfile[j + 1],\n",
    "                            'trips': int(matrix[i, j])\n",
    "                        })\n",
    "            \n",
    "            # Store bar data\n",
    "            for idx, (o_class, d_class) in enumerate(top_pair_keys):\n",
    "                if trips_array[idx] > 0:  # Only store non-zero values\n",
    "                    all_bar_data.append({\n",
    "                        'day': day,\n",
    "                        'hour': hour,\n",
    "                        'origin_class': o_class,\n",
    "                        'destination_class': d_class,\n",
    "                        'origin_class_name': IndexVodafone2UserProfile[o_class],\n",
    "                        'destination_class_name': IndexVodafone2UserProfile[d_class],\n",
    "                        'trips': int(trips_array[idx]),\n",
    "                        'pair_label': top_pair_labels[idx]\n",
    "                    })\n",
    "        \n",
    "        # === CREATE ANIMATIONS FOR THIS DAY ===\n",
    "        if len(daily_matrices) > 1:  # Only create animation if we have multiple time points\n",
    "            print(f\"  Creating animations for day: {day}\")\n",
    "            \n",
    "            # Create matrix animation\n",
    "            create_matrix_animation(daily_matrices, day, unique_hours, x_labels, y_labels, \n",
    "                                  animation_dir, IndexVodafone2UserProfile, max_trips_matrix)\n",
    "            \n",
    "            # Create bar animation\n",
    "            create_bar_animation(daily_bar_data, day, unique_hours, top_pair_labels, \n",
    "                               animation_dir, top_n, max_trips_bar)\n",
    "    \n",
    "    # === SAVE DATA TO PARQUET FILES ===\n",
    "    if all_matrix_data:\n",
    "        matrix_df = pl.DataFrame(all_matrix_data)\n",
    "        matrix_df.write_parquet(os.path.join(data_dir, \"od_matrix_data_daily_hourly.parquet\"))\n",
    "        print(f\"Saved matrix data: {len(all_matrix_data)} records\")\n",
    "    \n",
    "    if all_bar_data:\n",
    "        bar_df = pl.DataFrame(all_bar_data)\n",
    "        bar_df.write_parquet(os.path.join(data_dir, \"od_top_pairs_data_daily_hourly.parquet\"))\n",
    "        print(f\"Saved bar data: {len(all_bar_data)} records\")\n",
    "    \n",
    "    # === SAVE METADATA ===\n",
    "    metadata = {\n",
    "        'unique_days': unique_days,\n",
    "        'unique_hours': unique_hours,\n",
    "        'visitor_classes': list(IndexVodafone2UserProfile.items()),\n",
    "        'top_pairs': [(k, v) for k, v in zip(top_pair_keys, top_pair_labels)],\n",
    "        'total_matrix_plots': len(unique_days) * len(unique_hours),\n",
    "        'total_bar_plots': len(unique_days) * len(unique_hours),\n",
    "        'total_animations': len(unique_days) * 2,  # 2 animations per day (matrix + bar)\n",
    "        'processing_date': pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(data_dir, \"metadata.json\"), \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nProcessing complete!\")\n",
    "    print(f\"Matrix plots saved to: {matrix_dir}\")\n",
    "    print(f\"Bar plots saved to: {bar_dir}\")\n",
    "    print(f\"Animations saved to: {animation_dir}\")\n",
    "    print(f\"Data files saved to: {data_dir}\")\n",
    "    \n",
    "    return all_matrix_data, all_bar_data, metadata\n",
    "\n",
    "def create_matrix_animation(daily_matrices, day, unique_hours, x_labels, y_labels, \n",
    "                           animation_dir, IndexVodafone2UserProfile, max_trips):\n",
    "    \"\"\"Create and save matrix animation for a single day.\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Initialize the heatmap\n",
    "    im = ax.imshow(daily_matrices[0], cmap='YlOrRd', aspect='auto', \n",
    "                   vmin=0, vmax=max_trips if max_trips > 0 else 1)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label('Total Trips', rotation=270, labelpad=20)\n",
    "    \n",
    "    # Set labels\n",
    "    ax.set_xticks(range(4))\n",
    "    ax.set_yticks(range(4))\n",
    "    ax.set_xticklabels(x_labels, rotation=45)\n",
    "    ax.set_yticklabels(y_labels)\n",
    "    ax.set_xlabel('Destination Visitor Class')\n",
    "    ax.set_ylabel('Origin Visitor Class')\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    ax.set_xticks(np.arange(-0.5, 4, 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-0.5, 4, 1), minor=True)\n",
    "    ax.grid(which='minor', color='gray', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "    \n",
    "    # Add text annotations for values\n",
    "    text_objects = []\n",
    "    for i in range(4):\n",
    "        row_texts = []\n",
    "        for j in range(4):\n",
    "            text = ax.text(j, i, '', ha='center', va='center', \n",
    "                          color='black', fontweight='bold', fontsize=8)\n",
    "            row_texts.append(text)\n",
    "        text_objects.append(row_texts)\n",
    "    \n",
    "    title = ax.set_title('')\n",
    "    \n",
    "    def animate(frame):\n",
    "        hour = unique_hours[frame]\n",
    "        matrix = daily_matrices[frame]\n",
    "        \n",
    "        # Update the image data\n",
    "        im.set_array(matrix)\n",
    "        \n",
    "        # Update text annotations\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                value = matrix[i, j]\n",
    "                if value > 0:\n",
    "                    # Format large numbers with K/M notation\n",
    "                    if value >= 1000000:\n",
    "                        text_objects[i][j].set_text(f'{value/1000000:.1f}M')\n",
    "                    elif value >= 1000:\n",
    "                        text_objects[i][j].set_text(f'{value/1000:.1f}K')\n",
    "                    else:\n",
    "                        text_objects[i][j].set_text(f'{int(value)}')\n",
    "                else:\n",
    "                    text_objects[i][j].set_text('')\n",
    "        \n",
    "        # Update title\n",
    "        title.set_text(f'Daily Trips by Visitor Class - {day}, Hour: {hour}:00')\n",
    "        \n",
    "        return [im] + [text for row in text_objects for text in row] + [title]\n",
    "    \n",
    "    # Create animation\n",
    "    anim = animation.FuncAnimation(fig, animate, frames=len(daily_matrices), \n",
    "                                 interval=1000, blit=True, repeat=True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save animation\n",
    "    gif_path = os.path.join(animation_dir, f\"matrix_animation_{day}.gif\")\n",
    "    mp4_path = os.path.join(animation_dir, f\"matrix_animation_{day}.mp4\")\n",
    "    \n",
    "    try:\n",
    "        anim.save(gif_path, writer='pillow', fps=1)\n",
    "        print(f\"  Saved matrix animation (GIF): {gif_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not save GIF: {e}\")\n",
    "    \n",
    "    try:\n",
    "        anim.save(mp4_path, writer='ffmpeg', fps=1)\n",
    "        print(f\"  Saved matrix animation (MP4): {mp4_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not save MP4: {e}\")\n",
    "    \n",
    "    plt.close()\n",
    "    return anim\n",
    "\n",
    "def create_bar_animation(daily_bar_data, day, unique_hours, top_pair_labels, \n",
    "                        animation_dir, top_n, max_trips):\n",
    "    \"\"\"Create and save bar animation for a single day.\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Create initial bar plot\n",
    "    x_pos = np.arange(len(top_pair_labels))\n",
    "    bars = ax.bar(x_pos, daily_bar_data[0], alpha=0.7)\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_xlabel('Top Origin → Destination Visitor Class Pairs', fontsize=12)\n",
    "    ax.set_ylabel('Total Trips', fontsize=12)\n",
    "    ax.set_ylim(0, max_trips * 1.2 if max_trips > 0 else 1)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(top_pair_labels, rotation=45, ha='right')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    value_texts = []\n",
    "    for bar in bars:\n",
    "        text = ax.text(bar.get_x() + bar.get_width()/2., \n",
    "                      bar.get_height() + max_trips * 0.02 if max_trips > 0 else 0.02,\n",
    "                      '', ha='center', va='bottom', \n",
    "                      fontsize=9, fontweight='bold')\n",
    "        value_texts.append(text)\n",
    "    \n",
    "    title = ax.set_title('')\n",
    "    \n",
    "    def animate(frame):\n",
    "        hour = unique_hours[frame]\n",
    "        data = daily_bar_data[frame]\n",
    "        \n",
    "        # Update bar heights and colors\n",
    "        for bar, value in zip(bars, data):\n",
    "            bar.set_height(value)\n",
    "            \n",
    "            # Color bars based on value (gradient from light to dark)\n",
    "            if max_trips > 0:\n",
    "                intensity = min(1.0, value / (max_trips * 0.7))\n",
    "                bar.set_color(plt.cm.YlOrRd(intensity))\n",
    "            else:\n",
    "                bar.set_color('lightgray')\n",
    "        \n",
    "        # Update value labels\n",
    "        for bar, text, value in zip(bars, value_texts, data):\n",
    "            if value > 0:\n",
    "                if value >= 1000000:\n",
    "                    label = f'{value/1000000:.1f}M'\n",
    "                elif value >= 1000:\n",
    "                    label = f'{value/1000:.0f}K'\n",
    "                else:\n",
    "                    label = f'{int(value)}'\n",
    "                text.set_text(label)\n",
    "                text.set_position((bar.get_x() + bar.get_width()/2., \n",
    "                                 bar.get_height() + (max_trips * 0.02 if max_trips > 0 else 0.02)))\n",
    "            else:\n",
    "                text.set_text('')\n",
    "        \n",
    "        # Update title\n",
    "        title.set_text(f'Top {len(top_pair_labels)} O-D Pairs - {day}, Hour: {hour}:00')\n",
    "        \n",
    "        # Return as list (fix for animation)\n",
    "        return list(bars) + value_texts + [title]\n",
    "    \n",
    "    # Create animation\n",
    "    anim = animation.FuncAnimation(fig, animate, frames=len(daily_bar_data), \n",
    "                                 interval=1200, blit=True, repeat=True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save animation\n",
    "    gif_path = os.path.join(animation_dir, f\"bars_animation_{day}.gif\")\n",
    "    mp4_path = os.path.join(animation_dir, f\"bars_animation_{day}.mp4\")\n",
    "    \n",
    "    try:\n",
    "        anim.save(gif_path, writer='pillow', fps=0.8)\n",
    "        print(f\"  Saved bar animation (GIF): {gif_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not save GIF: {e}\")\n",
    "    \n",
    "    try:\n",
    "        anim.save(mp4_path, writer='ffmpeg', fps=0.8)\n",
    "        print(f\"  Saved bar animation (MP4): {mp4_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not save MP4: {e}\")\n",
    "    \n",
    "    plt.close()\n",
    "    return anim\n",
    "\n",
    "# Keep the existing static plot creation functions\n",
    "def create_and_save_matrix_plot(matrix, day, hour, x_labels, y_labels, \n",
    "                               output_dir, IndexVodafone2UserProfile):\n",
    "    \"\"\"Create and save a single matrix heatmap plot.\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Create heatmap\n",
    "    im = ax.imshow(matrix, cmap='YlOrRd', aspect='auto', \n",
    "                   vmin=0, vmax=matrix.max() if matrix.max() > 0 else 1)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label('Total Trips', rotation=270, labelpad=20)\n",
    "    \n",
    "    # Set labels\n",
    "    ax.set_xticks(range(4))\n",
    "    ax.set_yticks(range(4))\n",
    "    ax.set_xticklabels(x_labels, rotation=45)\n",
    "    ax.set_yticklabels(y_labels)\n",
    "    ax.set_xlabel('Destination Visitor Class')\n",
    "    ax.set_ylabel('Origin Visitor Class')\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    ax.set_xticks(np.arange(-0.5, 4, 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-0.5, 4, 1), minor=True)\n",
    "    ax.grid(which='minor', color='gray', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "    \n",
    "    # Add text annotations for values\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            value = matrix[i, j]\n",
    "            if value > 0:\n",
    "                # Format large numbers with K/M notation\n",
    "                if value >= 1000000:\n",
    "                    text = f'{value/1000000:.1f}M'\n",
    "                elif value >= 1000:\n",
    "                    text = f'{value/1000:.1f}K'\n",
    "                else:\n",
    "                    text = f'{int(value)}'\n",
    "                    \n",
    "                ax.text(j, i, text, ha='center', va='center', \n",
    "                       color='black', fontweight='bold', fontsize=8)\n",
    "    \n",
    "    # Set title\n",
    "    ax.set_title(f'Daily Trips by Visitor Class - {day}, Hour: {hour}:00', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    filename = f\"matrix_{day}_hour_{hour:02d}.png\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def create_and_save_bar_plot(trips_array, day, hour, top_pair_labels, \n",
    "                           output_dir, top_n):\n",
    "    \"\"\"Create and save a single bar plot.\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Create bar plot\n",
    "    x_pos = np.arange(len(top_pair_labels))\n",
    "    max_trips = trips_array.max() if trips_array.max() > 0 else 1\n",
    "    \n",
    "    bars = ax.bar(x_pos, trips_array, alpha=0.7)\n",
    "    \n",
    "    # Color bars based on value (gradient from light to dark)\n",
    "    for bar, value in zip(bars, trips_array):\n",
    "        if max_trips > 0:\n",
    "            intensity = min(1.0, value / (max_trips * 0.7))\n",
    "            bar.set_color(plt.cm.YlOrRd(intensity))\n",
    "        else:\n",
    "            bar.set_color('lightgray')\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_xlabel('Top Origin → Destination Visitor Class Pairs', fontsize=12)\n",
    "    ax.set_ylabel('Total Trips', fontsize=12)\n",
    "    ax.set_ylim(0, max_trips * 1.2)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(top_pair_labels, rotation=45, ha='right')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, trips_array):\n",
    "        if value > 0:\n",
    "            if value >= 1000000:\n",
    "                label = f'{value/1000000:.1f}M'\n",
    "            elif value >= 1000:\n",
    "                label = f'{value/1000:.0f}K'\n",
    "            else:\n",
    "                label = f'{int(value)}'\n",
    "            \n",
    "            ax.text(bar.get_x() + bar.get_width()/2., \n",
    "                   bar.get_height() + max_trips * 0.02,\n",
    "                   label, ha='center', va='bottom', \n",
    "                   fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Set title\n",
    "    ax.set_title(f'Top {len(top_pair_labels)} O-D Pairs - {day}, Hour: {hour}:00',\n",
    "                fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    filename = f\"bars_{day}_hour_{hour:02d}.png\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def create_summary_visualizations(base_dir_output, all_matrix_data, all_bar_data):\n",
    "    \"\"\"Create summary visualizations across all days and hours.\"\"\"\n",
    "    \n",
    "    summary_dir = os.path.join(base_dir_output, \"summary_plots\")\n",
    "    Path(summary_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if all_matrix_data:\n",
    "        matrix_df = pl.DataFrame(all_matrix_data)\n",
    "        \n",
    "        # === DAILY TOTALS HEATMAP ===\n",
    "        daily_totals = matrix_df.group_by(['day', 'origin_class', 'destination_class']).agg([\n",
    "            pl.col('trips').sum().alias('daily_trips')\n",
    "        ])\n",
    "        \n",
    "        # Convert to pivot table for heatmap\n",
    "        pivot_data = daily_totals.to_pandas().pivot_table(\n",
    "            values='daily_trips', \n",
    "            index=['origin_class', 'destination_class'], \n",
    "            columns='day', \n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        # Create heatmap\n",
    "        fig, ax = plt.subplots(figsize=(20, 12))\n",
    "        sns.heatmap(pivot_data, annot=True, fmt='g', cmap='YlOrRd', \n",
    "                   cbar_kws={'label': 'Total Daily Trips'})\n",
    "        plt.title('Daily Trips by O-D Pairs Across All Days', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Day', fontsize=12)\n",
    "        plt.ylabel('Origin-Destination Class Pairs', fontsize=12)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(summary_dir, \"daily_totals_heatmap.png\"), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # === HOURLY PATTERNS ===\n",
    "        hourly_totals = matrix_df.group_by(['hour', 'origin_class', 'destination_class']).agg([\n",
    "            pl.col('trips').sum().alias('hourly_trips')\n",
    "        ])\n",
    "        \n",
    "        hourly_pivot = hourly_totals.to_pandas().pivot_table(\n",
    "            values='hourly_trips',\n",
    "            index=['origin_class', 'destination_class'],\n",
    "            columns='hour',\n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(16, 10))\n",
    "        sns.heatmap(hourly_pivot, annot=True, fmt='g', cmap='YlOrRd',\n",
    "                   cbar_kws={'label': 'Total Hourly Trips'})\n",
    "        plt.title('Hourly Trip Patterns by O-D Pairs', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Hour of Day', fontsize=12)\n",
    "        plt.ylabel('Origin-Destination Class Pairs', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(summary_dir, \"hourly_patterns_heatmap.png\"),\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"Summary visualizations saved to: {summary_dir}\")\n",
    "\n",
    "# === MAIN EXECUTION FUNCTION ===\n",
    "def run_complete_od_analysis_with_animations(df_od, IndexVodafone2UserProfile, base_dir_output_save, \n",
    "                                            str_period_id_presenze=\"PERIOD_ID\", top_n=10):\n",
    "    \"\"\"\n",
    "    Main function to run the complete daily O-D analysis with animations.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Starting comprehensive O-D visualization analysis with animations...\")\n",
    "    \n",
    "    # Create daily visualizations and animations\n",
    "    all_matrix_data, all_bar_data, metadata = create_daily_od_visualizations_with_animations(\n",
    "        df_od=df_od,\n",
    "        IndexVodafone2UserProfile=IndexVodafone2UserProfile,\n",
    "        base_dir_output=base_dir_output_save,\n",
    "        str_period_id_presenze=str_period_id_presenze,\n",
    "        top_n=top_n\n",
    "    )\n",
    "    \n",
    "    # Create summary visualizations\n",
    "    create_summary_visualizations(base_dir_output_save, all_matrix_data, all_bar_data)\n",
    "    \n",
    "    print(\"Complete analysis with animations finished!\")\n",
    "    return all_matrix_data, all_bar_data, metadata\n",
    "\n",
    "# === USAGE EXAMPLE ===\n",
    "# Set up output directory\n",
    "base_dir_output_save = os.path.join(config[str_dir_output], \"complete_od_analysis_with_animations\")\n",
    "\n",
    "# Run the complete analysis with animations\n",
    "all_matrix_data, all_bar_data, metadata = run_complete_od_analysis_with_animations(\n",
    "    df_od=df_od,\n",
    "    IndexVodafone2UserProfile=IndexVodafone2UserProfile,\n",
    "    base_dir_output_save=base_dir_output_save,\n",
    "    str_period_id_presenze=str_period_id_presenze,\n",
    "    top_n=12  # Show top 12 O-D pairs in bar charts\n",
    ")\n",
    "\n",
    "print(\"Complete O-D analysis with animations completed successfully!\")\n",
    "print(f\"Results saved in: {base_dir_output_save}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dac3c5ca",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
